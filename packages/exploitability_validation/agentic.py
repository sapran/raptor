"""
Agentic Integration Module

Provides functions to integrate exploitability validation into the RAPTOR agentic pipeline.
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any

logger = logging.getLogger(__name__)

# Map common rule patterns to vuln_type enum
VULN_TYPE_MAP = {
    # Injection
    'sql': 'sql_injection',
    'sqli': 'sql_injection',
    'command': 'command_injection',
    'cmd': 'command_injection',
    'exec': 'command_injection',
    'shell': 'command_injection',
    'xss': 'xss',
    'cross-site': 'xss',
    'ssrf': 'ssrf',
    'path-traversal': 'path_traversal',
    'path_traversal': 'path_traversal',
    'traversal': 'path_traversal',
    'lfi': 'path_traversal',
    'rfi': 'path_traversal',
    # Deserialization
    'deseriali': 'deserialization',
    'pickle': 'deserialization',
    'yaml.load': 'deserialization',
    'marshal': 'deserialization',
    # Memory corruption
    'buffer': 'buffer_overflow',
    'overflow': 'buffer_overflow',
    'strcpy': 'buffer_overflow',
    'strcat': 'buffer_overflow',
    'sprintf': 'buffer_overflow',
    'gets': 'buffer_overflow',
    'format-string': 'format_string',
    'format_string': 'format_string',
    'printf': 'format_string',
    'use-after-free': 'use_after_free',
    'uaf': 'use_after_free',
    'double-free': 'double_free',
    'integer-overflow': 'integer_overflow',
    'int-overflow': 'integer_overflow',
    # Secrets/Crypto
    'secret': 'hardcoded_secret',
    'credential': 'hardcoded_secret',
    'password': 'hardcoded_secret',
    'api-key': 'hardcoded_secret',
    'apikey': 'hardcoded_secret',
    'crypto': 'weak_crypto',
    'weak-hash': 'weak_crypto',
    'md5': 'weak_crypto',
    'sha1': 'weak_crypto',
}


def classify_vuln_type(rule_id: str, message: str) -> str:
    """Classify vulnerability type from rule ID and message."""
    text = (rule_id + ' ' + message).lower()
    for pattern, vuln_type in VULN_TYPE_MAP.items():
        if pattern in text:
            return vuln_type
    return 'other'


def convert_sarif_to_findings(sarif_files: list, target_path: str) -> dict:
    """
    Convert SARIF findings to the validation pipeline's findings format.

    Args:
        sarif_files: List of Path objects to SARIF files
        target_path: Base path of the scanned repository

    Returns:
        Findings dict in validation pipeline format
    """
    def extract_function_name(uri: str, line: int) -> str:
        """Try to extract function name from file context."""
        return f"function_at_line_{line}"

    findings_list = []
    finding_id = 0
    seen = set()  # Deduplicate findings

    for sarif_path in sarif_files:
        try:
            with open(sarif_path, 'r') as f:
                sarif_data = json.load(f)
        except (json.JSONDecodeError, FileNotFoundError):
            continue

        for run in sarif_data.get('runs', []):
            for result in run.get('results', []):
                rule_id = result.get('ruleId', 'unknown')
                message = result.get('message', {}).get('text', '')

                for location in result.get('locations', []):
                    phys = location.get('physicalLocation', {})
                    artifact = phys.get('artifactLocation', {})
                    region = phys.get('region', {})

                    uri = artifact.get('uri', '')
                    line = region.get('startLine', 1)

                    # Create dedup key
                    dedup_key = f"{uri}:{line}:{rule_id}"
                    if dedup_key in seen:
                        continue
                    seen.add(dedup_key)

                    # Normalize file path
                    file_path = uri
                    if file_path.startswith('file://'):
                        file_path = file_path[7:]

                    finding_id += 1
                    findings_list.append({
                        "id": f"FIND-{finding_id:04d}",
                        "file": file_path,
                        "function": extract_function_name(uri, line),
                        "line": line,
                        "vuln_type": classify_vuln_type(rule_id, message),
                        "status": "not_disproven",
                        "source_rule": rule_id,
                        "source_message": message[:500],
                        "poc": None,
                        "proof": None,
                        "sanity_check": None,
                        "ruling": None,
                        "feasibility": None,
                        "disproved_because": None,
                        "candidate_reasoning": f"Detected by static analysis rule: {rule_id}",
                        "final_status": None
                    })

    return {
        "stage": "A",
        "timestamp": datetime.now().isoformat(),
        "target_path": target_path,
        "vuln_type_focus": None,
        "findings": findings_list,
        "summary": {
            "total_input": len(findings_list),
            "confirmed": 0,
            "ruled_out": 0
        }
    }


def run_validation_phase(
    repo_path: str,
    out_dir: Path,
    sarif_files: List[Path],
    total_findings: int,
    vuln_type: Optional[str] = None,
    binary_path: Optional[str] = None,
    skip_validation: bool = False,
    skip_feasibility: bool = True,
    llm_available: bool = False,
) -> Tuple[Dict[str, Any], int]:
    """
    Run the exploitability validation phase of the agentic pipeline.

    Args:
        repo_path: Path to the repository being analyzed
        out_dir: Output directory for validation results
        sarif_files: List of SARIF files from scanning phase
        total_findings: Total number of findings from scanning
        vuln_type: Optional vulnerability type to focus on
        binary_path: Optional path to binary for feasibility analysis
        skip_validation: Skip validation entirely
        skip_feasibility: Skip Stage E feasibility analysis
        llm_available: Whether an LLM is available for full validation

    Returns:
        Tuple of (validation_result dict, validated_findings count)
    """
    validation_result = {}
    validated_findings = total_findings

    if skip_validation:
        print("\n⏭️  Skipping exploitability validation (--skip-validation)")
        logger.info("Exploitability validation skipped by user request")
        return validation_result, validated_findings

    if total_findings == 0:
        print("\n⏭️  Skipping exploitability validation (no findings to validate)")
        logger.info("Exploitability validation skipped - no findings")
        return validation_result, validated_findings

    binary_provided = bool(binary_path)

    if not llm_available and not binary_provided:
        # No LLM and no binary = just deduplicate
        print("\n" + "=" * 70)
        print("PHASE 2: DEDUPLICATION (no LLM configured)")
        print("=" * 70)
        print("\nNo LLM API key found - performing deduplication only.")
        print("For full validation, set ANTHROPIC_API_KEY or OPENAI_API_KEY.\n")

        validation_out = out_dir / "validation"
        validation_out.mkdir(exist_ok=True)

        print("[*] Deduplicating SARIF findings...")
        converted_findings = convert_sarif_to_findings(sarif_files, str(repo_path))
        findings_file_path = validation_out / "deduplicated_findings.json"
        with open(findings_file_path, 'w') as f:
            json.dump(converted_findings, f, indent=2, default=str)

        unique_count = len(converted_findings['findings'])
        duplicates_removed = total_findings - unique_count
        print(f"    ✓ {total_findings} findings → {unique_count} unique ({duplicates_removed} duplicates removed)")

        validated_findings = unique_count
        validation_result = {
            'completed': True,
            'mode': 'deduplication_only',
            'findings': converted_findings,
            'duplicates_removed': duplicates_removed
        }
        logger.info(f"Deduplication complete: {unique_count}/{total_findings} unique findings")

        print(f"\n{'=' * 70}")
        print(f"✓ PHASE 2 COMPLETE (deduplication only)")
        print(f"{'=' * 70}")
        print(f"Original findings: {total_findings}")
        print(f"Unique findings: {unique_count}")
        print(f"Duplicates removed: {duplicates_removed}")

        return validation_result, validated_findings

    # Full validation with LLM
    print("\n" + "=" * 70)
    print("PHASE 2: EXPLOITABILITY VALIDATION")
    print("=" * 70)
    print("\nValidating findings are real, reachable, and exploitable...")
    print("This filters hallucinations and unreachable code paths.\n")

    try:
        from .orchestrator import run_validation

        validation_out = out_dir / "validation"
        validation_out.mkdir(exist_ok=True)

        # Convert SARIF findings to validation pipeline format
        print("[*] Converting SARIF findings to validation format...")
        converted_findings = convert_sarif_to_findings(sarif_files, str(repo_path))
        findings_file_path = validation_out / "sarif_findings.json"
        with open(findings_file_path, 'w') as f:
            json.dump(converted_findings, f, indent=2, default=str)
        print(f"    ✓ Converted {len(converted_findings['findings'])} findings from SARIF")

        print("[*] Running validation pipeline...")
        state = run_validation(
            target_path=str(repo_path),
            vuln_type=vuln_type,
            binary_path=binary_path,
            findings_file=str(findings_file_path),
            skip_feasibility=skip_feasibility,
            workdir=str(validation_out),
        )

        # Extract results from state
        if state.checklist:
            func_count = state.checklist.get('total_functions', 0)
            file_count = state.checklist.get('total_files', 0)
            print(f"    ✓ Stage 0: Found {func_count} functions across {file_count} files")

        if state.findings:
            findings = state.findings.get('findings', [])
            total_input = len(findings)
            confirmed = sum(1 for f in findings if f.get('status') == 'confirmed')
            disproven = sum(1 for f in findings if f.get('status') == 'disproven')
            ruled_out = sum(1 for f in findings if f.get('status') == 'ruled_out')
            final_confirmed = sum(1 for f in findings if f.get('final_status') in
                          ('EXPLOITABLE', 'CONFIRMED_CONSTRAINED', 'CONFIRMED_BLOCKED', 'CONFIRMED'))

            print(f"    ✓ Input: {total_input} findings from SARIF")
            print(f"    ✓ Stage C (Sanity): {confirmed} confirmed, {disproven} failed verification")
            print(f"    ✓ Stage D (Ruling): {final_confirmed} exploitable, {ruled_out} ruled out")
            validated_findings = final_confirmed if final_confirmed > 0 else confirmed

        validation_result = {
            'completed': True,
            'mode': 'full_validation',
            'checklist': state.checklist,
            'findings': state.findings,
            'stage_results': {s.name: r.status.value for s, r in state.stage_results.items()}
        }

        print(f"\n{'=' * 70}")
        print(f"✓ PHASE 2 COMPLETE")
        print(f"{'=' * 70}")
        print(f"Original findings: {total_findings}")
        print(f"Validated findings: {validated_findings}")
        if total_findings > 0:
            reduction = ((total_findings - validated_findings) / total_findings) * 100
            print(f"Noise reduction: {reduction:.1f}%")

        logger.info(f"Exploitability validation complete: {validated_findings}/{total_findings} findings validated")

    except ImportError as e:
        print(f"\n⚠️  Exploitability validation module not available: {e}")
        print("    Continuing with all findings...")
        logger.warning(f"Exploitability validation import failed: {e}")
    except Exception as e:
        print(f"\n⚠️  Exploitability validation failed: {e}")
        print("    Continuing with all findings...")
        logger.error(f"Exploitability validation error: {e}")

    return validation_result, validated_findings
