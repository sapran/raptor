#!/usr/bin/env python3
"""
Exploit Feasibility - Public API

This module provides clean, user-facing functions for exploit feasibility analysis.
Analyzes both mitigations AND exploitation factors to determine what's possible.

Usage:
    from packages.exploit_feasibility import analyze_binary, check_exploit_viability

    # Run full analysis
    result = analyze_binary("/path/to/binary", output_dir="/path/to/out")
    print(result['summary'])

    # Quick viability check
    viable, reason = check_exploit_viability("/path/to/binary", "format_string")
"""

from pathlib import Path
from typing import Dict, Optional, Tuple, Any

from core.logging import get_logger

logger = get_logger()


# =============================================================================
# Auto-Profile Selection
# =============================================================================

def _get_profile_for_vuln_type(vuln_type: str = None, binary_path: str = None):
    """
    Auto-select appropriate analysis profile based on vulnerability type.

    Web vulnerabilities (SQLi, XSS, SSRF, etc.) automatically use WebApplicationStrategy
    which skips irrelevant memory mitigation checks.

    Args:
        vuln_type: Vulnerability type string (e.g., "sql_injection", "xss")
        binary_path: Path to binary (for local/binary contexts)

    Returns:
        TargetProfile appropriate for the vulnerability type
    """
    from .profiles import create_local_profile, create_web_profile
    from .vuln_types import VulnerabilityType

    # If no vuln_type, default to local binary analysis
    if not vuln_type:
        return create_local_profile(binary_path)

    # Normalize vuln_type to lowercase
    vuln_type_lower = vuln_type.lower().strip()

    # Web vulnerability types - memory mitigations don't apply
    WEB_VULN_TYPES = {
        'sql_injection', 'sqli', 'sql',
        'xss', 'cross_site_scripting',
        'ssrf', 'server_side_request_forgery',
        'path_traversal', 'lfi', 'rfi', 'directory_traversal',
        'command_injection', 'rce', 'os_command_injection',
        'xxe', 'xml_external_entity',
        'ssti', 'template_injection',
        'idor', 'insecure_direct_object_reference',
        'csrf', 'cross_site_request_forgery',
        'open_redirect',
        'deserialization', 'insecure_deserialization',
    }

    # Check if it's a web vulnerability
    if vuln_type_lower in WEB_VULN_TYPES:
        logger.debug(f"Auto-selected WebApplicationStrategy for vuln_type: {vuln_type}")
        return create_web_profile()

    # Also check VulnerabilityType enum if it's a valid enum value
    try:
        vt_enum = VulnerabilityType(vuln_type_lower)
        if VulnerabilityType.is_web(vt_enum):
            logger.debug(f"Auto-selected WebApplicationStrategy for VulnerabilityType: {vt_enum}")
            return create_web_profile()
    except ValueError:
        pass  # Not a valid VulnerabilityType enum value

    # Default to local binary analysis
    return create_local_profile(binary_path)


# =============================================================================
# Public API Functions
# =============================================================================

def analyze_binary(binary_path: str, output_dir: str = None,
                   vuln_type: str = None, extended: bool = True) -> Dict[str, Any]:
    """
    Run mitigation analysis on a binary.

    Args:
        binary_path: Path to target binary
        output_dir: Directory to save analysis results (optional)
        vuln_type: Vulnerability type for specific checks (optional)
        extended: Run extended analysis (libc offsets, ROP gadgets, etc.)

    Returns:
        Dictionary with analysis results:
        {
            'verdict': 'exploitable' | 'likely_exploitable' | 'blocked' | 'requires_environment',
            'summary': str,  # Human-readable summary
            'protections': {'relro': bool, 'pie': bool, 'nx': bool, 'canary': bool, ...},
            'blockers': [str, ...],  # Issues that block exploitation
            'warnings': [str, ...],  # Issues that complicate exploitation
            'suggestions': [str, ...],  # Bypass suggestions
            'constraints': {  # Architectural constraints
                'strcpy_rop_viable': bool,
                'null_byte_position': int,
                'blocked_techniques': [str, ...],
                'viable_techniques': [str, ...],
            },
            'libc': {'version': str, 'system_offset': int, ...},  # If extended
            'rop_gadgets': {'pop_rdi': int, 'ret': int, ...},  # If extended
        }
    """
    from .analyzer import FeasibilityAnalyzer, ExploitabilityVerdict
    from .context import ExploitationConstraints

    binary_path = Path(binary_path) if binary_path else None

    if binary_path and not binary_path.exists():
        return {
            'verdict': 'error',
            'summary': f'Binary not found: {binary_path}',
            'error': f'File does not exist: {binary_path}'
        }

    # Auto-select profile based on vulnerability type (web vulns skip memory mitigations)
    profile = _get_profile_for_vuln_type(vuln_type, str(binary_path) if binary_path else None)

    # Run analysis with appropriate profile
    analyzer = FeasibilityAnalyzer(binary_path=binary_path, profile=profile)
    report = analyzer.full_analysis(vuln_type=vuln_type, extended=extended)

    # Build clean result dict
    result = {
        'verdict': report.verdict.value,
        'summary': report.summary(),
        'protections': dict(report.binary_protections),
        'blockers': list(report.blockers),
        'warnings': list(report.warnings),
        'suggestions': list(report.bypass_suggestions),
    }

    # Add runtime-verified glibc info
    result['glibc_n_disabled'] = getattr(report, 'glibc_n_disabled', None)
    result['glibc_version'] = getattr(report, 'glibc_version', None)

    # Add raw checksec output for LLM context
    if report.raw_checksec:
        result['raw_checksec'] = report.raw_checksec

    # Add comprehensive glibc mitigations
    if report.glibc_mitigations:
        gm = report.glibc_mitigations
        result['glibc_mitigations'] = {
            'version': gm.version,
            'pointer_mangling': gm.pointer_mangling,
            'tcache_enabled': gm.tcache_enabled,
            'tcache_key': gm.tcache_key,
            'safe_linking': gm.safe_linking,
            'hooks_removed': gm.hooks_removed,
            'vtable_verification': gm.vtable_verification,
            'format_n_disabled': gm.format_n_disabled,
            'format_n_verified': gm.format_n_verified,
            'blockers': [{'name': m.name, 'description': m.description,
                         'blocked_techniques': m.blocked_techniques,
                         'alternatives': m.alternatives}
                        for m in gm.get_blockers()],
            'primitive_requirements': [{'name': m.name, 'description': m.description,
                                       'required_primitive': m.required_primitive,
                                       'bypass_requirements': m.bypass_requirements}
                                      for m in gm.get_primitive_requirements()],
            'complications': [{'name': m.name, 'description': m.description}
                             for m in gm.get_complications()],
        }

    # Add comprehensive kernel mitigations
    if report.kernel_mitigations_detailed:
        km = report.kernel_mitigations_detailed
        result['kernel_mitigations'] = {
            'version': km.kernel_version,
            'aslr_level': km.aslr_level,
            'mmap_min_addr': km.mmap_min_addr,
            'kptr_restrict': km.kptr_restrict,
            'dmesg_restrict': km.dmesg_restrict,
            'ptrace_scope': km.ptrace_scope,
            'unprivileged_bpf_disabled': km.unprivileged_bpf_disabled,
            'unprivileged_userns_clone': km.unprivileged_userns_clone,
            'modules_disabled': km.modules_disabled,
            'protected_symlinks': km.protected_symlinks,
            'protected_hardlinks': km.protected_hardlinks,
            'suid_dumpable': km.suid_dumpable,
            'perf_event_paranoid': km.perf_event_paranoid,
            'blockers': [{'name': m.name, 'sysctl': m.sysctl_path, 'value': m.current_value,
                         'description': m.description, 'blocked_techniques': m.blocked_techniques,
                         'alternatives': m.alternatives}
                        for m in km.get_blockers()],
            'primitive_requirements': [{'name': m.name, 'sysctl': m.sysctl_path, 'value': m.current_value,
                                       'description': m.description, 'required_primitive': m.required_primitive,
                                       'bypass_requirements': m.bypass_requirements}
                                      for m in km.get_primitive_requirements()],
            'complications': [{'name': m.name, 'sysctl': m.sysctl_path, 'value': m.current_value,
                              'description': m.description}
                             for m in km.get_complications()],
        }

    # Add detected input handlers (crucial for constraint analysis)
    if report.detected_input_handlers:
        result['input_handlers'] = list(report.detected_input_handlers)

    # Add full payload constraints for exploit development
    if report.payload_constraints:
        pc = report.payload_constraints
        result['payload_constraints'] = {
            'bad_bytes': pc.bad_bytes,
            'bad_byte_reasons': {str(k): v for k, v in pc.bad_byte_reasons.items()},
            'input_handler': pc.input_handler,
            'max_length': pc.max_length,
            'max_embeddable_addresses': pc.max_embeddable_addresses,
            # Charset constraints
            'must_be_printable': pc.must_be_printable,
            'must_be_alphanumeric': pc.must_be_alphanumeric,
            'allowed_charset': pc.allowed_charset,
            'newline_terminates': pc.newline_terminates,
            # Encoding guidance
            'encoding_notes': pc.encoding_notes,
            'encoding_suggestions': pc.encoding_suggestions,
        }

    # Add exploit primitives (what the vulnerability gives you)
    if report.exploit_primitives:
        ep = report.exploit_primitives
        result['exploit_primitives'] = {
            'vuln_type': ep.name,
            # Read/Write capabilities
            'arbitrary_read': ep.arbitrary_read,
            'arbitrary_write': ep.arbitrary_write,
            'relative_read': ep.relative_read,    # OOB read
            'relative_write': ep.relative_write,  # OOB write
            'limited_write': ep.limited_write,    # Sequential overflow
            # Control flow
            'control_rip': ep.control_rip,
            'control_rsp': ep.control_rsp,  # Stack pivot possible
            # Other capabilities
            'info_leak': ep.info_leak,
            'heap_control': ep.heap_control,
            # Write characteristics (for feasibility)
            'write_size': ep.write_size,
            'write_count': ep.write_count,
            # Requirements
            'requires_leak': ep.requires_leak,
            'requires_heap_feng_shui': ep.requires_heap_feng_shui,
            'notes': ep.notes,
        }

    # Add constraints if available
    if report.exploitation_constraints:
        ec = report.exploitation_constraints
        result['constraints'] = {
            'arch': ec.arch,
            'input_handler': ec.input_handler,  # Which handler drove these constraints
            'strcpy_rop_viable': ec.strcpy_rop_viable,
            'null_byte_position': ec.null_byte_position,
            'max_strcpy_bytes': ec.max_strcpy_bytes,
            'blocked_techniques': list(ec.blocked_techniques),
            'viable_techniques': list(ec.viable_techniques),
        }

    # Add libc info if available
    if report.libc_info:
        result['libc'] = {
            'version': report.glibc_version,
            'path': report.libc_info.path,
        }
        if report.libc_info.system_offset:
            result['libc']['system_offset'] = report.libc_info.system_offset
        if report.libc_info.bin_sh_offset:
            result['libc']['bin_sh_offset'] = report.libc_info.bin_sh_offset
        if report.libc_info.one_gadgets:
            result['libc']['one_gadgets'] = list(report.libc_info.one_gadgets)
        # Add detailed one_gadget info with constraints
        if report.libc_info.one_gadgets_detailed:
            result['libc']['one_gadgets_detailed'] = [
                {
                    'offset': g.offset,
                    'constraints': g.constraints,
                    'description': g.description,
                    'partial_overwrite_viable': g.partial_overwrite_viable,
                    'bytes_from_base': g.bytes_from_base,
                }
                for g in report.libc_info.one_gadgets_detailed
            ]
            result['libc']['has_partial_overwrite_gadget'] = report.libc_info.has_partial_overwrite_gadget
            if report.libc_info.best_partial_gadget:
                result['libc']['best_partial_gadget'] = {
                    'offset': report.libc_info.best_partial_gadget.offset,
                    'constraints': report.libc_info.best_partial_gadget.constraints,
                }

    # Add format string context
    if report.format_string_call_count > 0:
        result['format_string_context'] = {
            'call_count': report.format_string_call_count,
            'sinks': dict(report.format_string_sinks),
            'single_shot': report.single_shot_format_string,
            # %n write practical limits
            'write_limits': {
                '%n': '4 bytes, but value = char_count (impractical for large values)',
                '%hn': '2 bytes, practical up to 65535 chars',
                '%hhn': '1 byte, practical (max 255 chars per write)',
                'note': 'For large values, use multiple %hn/%hhn writes to construct byte-by-byte'
            }
        }
        if report.single_shot_format_string:
            result['format_string_context']['warning'] = (
                'Only one printf call - cannot chain writes across multiple calls. '
                'Must complete exploitation in single format string.'
            )

    # Add input handler constraints
    if report.payload_constraints and report.payload_constraints.max_embeddable_addresses != 0:
        if 'constraints' not in result:
            result['constraints'] = {}
        addr_limit = report.payload_constraints.max_embeddable_addresses
        result['constraints']['max_embeddable_addresses'] = addr_limit
        if addr_limit == 1:
            result['constraints']['address_warning'] = (
                'Only 1 address can be embedded (null byte terminates). '
                'Format string attacks requiring 2+ addresses (e.g., target + value) are blocked. '
                'Use: partial overwrite, stack-based addresses, or %n with existing stack pointers.'
            )
        elif addr_limit == -1:
            result['constraints']['address_note'] = 'Multiple addresses can be embedded (binary-safe input)'

    # Add detailed input constraint analysis
    # This provides actionable blocked/viable/conditional categorization
    if report.exploitation_constraints or report.detected_input_handlers:
        from .constraints import analyze_input_constraints

        handler = ""
        if report.exploitation_constraints:
            handler = report.exploitation_constraints.input_handler
        elif report.detected_input_handlers:
            # Prioritize most constraining handlers
            for h in ['strcpy', 'strcat', 'sprintf', 'gets', 'scanf']:
                if h in report.detected_input_handlers:
                    handler = h
                    break
            if not handler and report.detected_input_handlers:
                handler = list(report.detected_input_handlers)[0]

        if handler:
            has_full_relro = report.binary_protections.get('full_relro', False)
            format_n_works = not getattr(report, 'glibc_n_disabled', True)

            input_analysis = analyze_input_constraints(
                handler=handler,
                arch=report.exploitation_constraints.arch if report.exploitation_constraints else "x86_64",
                has_pie=report.binary_protections.get('pie', False),
                has_full_relro=has_full_relro,
                format_n_works=format_n_works,
            )
            result['input_constraint_analysis'] = input_analysis.to_dict()

    # Add libc fingerprinting potential assessment
    # Helps determine if pursuing a libc leak is worthwhile
    if report.elf_structure:
        from .constraints import assess_libc_fingerprinting

        elf = report.elf_structure
        plt_symbols = list(elf.plt_entries.keys()) if elf.plt_entries else []
        got_symbols = list(elf.got_entries.keys()) if elf.got_entries else []

        if plt_symbols or got_symbols:
            fingerprint_potential = assess_libc_fingerprinting(plt_symbols, got_symbols)
            result['libc_fingerprinting'] = fingerprint_potential.to_dict()

        # Add full ELF structure for exploit development
        result['elf_structure'] = {
            'got_plt_addr': elf.got_plt_addr,
            'got_plt_size': elf.got_plt_size,
            'fini_array_addr': elf.fini_array_addr,
            'fini_array_size': elf.fini_array_size,
            'init_array_addr': elf.init_array_addr,
            'init_array_size': elf.init_array_size,
            'bss_addr': elf.bss_addr,
            'bss_size': elf.bss_size,
            'plt_stub_size': elf.plt_stub_size,  # For ret2plt calculations
            'got_entries': dict(elf.got_entries) if elf.got_entries else {},
            'plt_entries': dict(elf.plt_entries) if elf.plt_entries else {},
        }

    # Add address space info if available (for exploit development)
    if report.address_space:
        addr = report.address_space
        result['address_space'] = {
            'binary_base_sample': addr.binary_base_sample,
            'libc_base_sample': addr.libc_base_sample,
            'stack_sample': addr.stack_sample,
            'heap_sample': addr.heap_sample,
            'ld_base_sample': addr.ld_base_sample,  # For ret2dlresolve
        }

    # Add bad byte impact analysis for each viable target
    if report.binary_specific and report.binary_specific.targets:
        from .constraints import analyze_bad_byte_impact

        bad_bytes = [0x00]  # Default
        if report.payload_constraints and report.payload_constraints.bad_bytes:
            bad_bytes = report.payload_constraints.bad_bytes

        bad_byte_impacts = []
        for target in report.binary_specific.targets:
            if target.address:
                impact = analyze_bad_byte_impact(
                    target_name=target.name,
                    address=target.address,
                    bad_bytes=bad_bytes,
                    technique="full_write" if not target.partial_overwrite_viable else "partial_overwrite",
                )
                if impact.bad_bytes_found:  # Only include targets with bad byte issues
                    bad_byte_impacts.append(impact.to_dict())

        if bad_byte_impacts:
            result['bad_byte_analysis'] = bad_byte_impacts

    # Add libc 2-byte gadgets if found
    if report.libc_2byte_gadgets:
        result['libc_2byte_gadgets'] = report.libc_2byte_gadgets

    # Add ROP gadgets if available (all gadgets for exploit development)
    if report.rop_gadgets:
        rg = report.rop_gadgets
        gadgets = {
            'total': rg.total_gadgets,
            'usable': rg.usable_gadgets,
            'filtered_by_bad_bytes': rg.filtered_by_bad_bytes,
            'filtered_by_charset': rg.filtered_by_charset,
            # Charset-specific counts for constrained payloads
            'printable_gadgets': rg.printable_gadgets,
            'alphanumeric_gadgets': rg.alphanumeric_gadgets,
        }
        # Essential gadgets for ret2libc
        if rg.pop_rdi_ret:
            gadgets['pop_rdi'] = rg.pop_rdi_ret
        if rg.pop_rsi_ret:
            gadgets['pop_rsi'] = rg.pop_rsi_ret
        if rg.pop_rdx_ret:
            gadgets['pop_rdx'] = rg.pop_rdx_ret
        if rg.pop_rax_ret:
            gadgets['pop_rax'] = rg.pop_rax_ret
        if rg.ret:
            gadgets['ret'] = rg.ret
        # Additional gadgets for advanced techniques
        if rg.pop_rbx_ret:
            gadgets['pop_rbx'] = rg.pop_rbx_ret  # ret2csu
        if rg.pop_rcx_ret:
            gadgets['pop_rcx'] = rg.pop_rcx_ret
        if rg.pop_rbp_ret:
            gadgets['pop_rbp'] = rg.pop_rbp_ret  # Stack pivot, ret2csu
        if rg.pop_rsp_ret:
            gadgets['pop_rsp'] = rg.pop_rsp_ret  # Stack pivot
        if rg.leave_ret:
            gadgets['leave_ret'] = rg.leave_ret  # Stack pivot
        if rg.syscall_ret:
            gadgets['syscall'] = rg.syscall_ret  # Syscall chain
        if rg.call_rax:
            gadgets['call_rax'] = rg.call_rax  # Function pointer calls
        # Include full gadget list for custom chains (limited to avoid huge output)
        if rg.all_gadgets:
            gadgets['all_gadgets'] = rg.all_gadgets[:100]  # First 100 for custom chains

        # ret2csu viability check (needs pop_rbx, pop_rbp + __libc_csu_init gadgets)
        ret2csu_viable = bool(rg.pop_rbx_ret and rg.pop_rbp_ret)
        gadgets['ret2csu_viable'] = ret2csu_viable
        if ret2csu_viable:
            gadgets['ret2csu_note'] = 'pop rbx + pop rbp found - check for __libc_csu_init gadgets'

        # Stack pivot viability
        gadgets['stack_pivot_viable'] = bool(rg.leave_ret or rg.pop_rsp_ret)

        result['rop_gadgets'] = gadgets

    # Add binary-specific analysis (concrete targets and technique viability)
    if report.binary_specific:
        bs = report.binary_specific
        result['binary_specific'] = {
            'protections': {
                'full_relro': bs.has_full_relro,
                'partial_relro': bs.has_partial_relro,
                'pie': bs.has_pie,
                'nx': bs.has_nx,
                'canary': bs.has_canary,
            },
            'viable_targets': [
                {
                    'name': t.name,
                    'address': hex(t.address),
                    'type': t.target_type,
                    'technique': t.technique,
                    'requires': t.requires,
                    'priority': t.priority,
                    'notes': t.notes,
                    'has_bad_bytes': t.has_bad_bytes,
                    # Add partial overwrite feasibility for relevant targets
                    'current_value_type': t.current_value_type if t.current_value_type else None,
                    'target_value_type': t.target_value_type if t.target_value_type else None,
                    'partial_overwrite_viable': t.partial_overwrite_viable,
                    'partial_overwrite_bytes_needed': t.partial_overwrite_bytes_needed if t.partial_overwrite_bytes_needed else None,
                }
                for t in bs.get_viable_targets()
            ],
            'blocked_targets': [
                {
                    'name': t.name,
                    'address': hex(t.address),
                    'blocked_by': t.blocked_by,
                }
                for t in bs.targets if not t.viable
            ],
            'viable_techniques': list(bs.viable_techniques),
            'blocked_techniques': dict(bs.blocked_techniques),
        }

        # Add best target recommendation
        best = bs.get_best_target()
        if best:
            result['binary_specific']['best_target'] = {
                'name': best.name,
                'address': hex(best.address),
                'requires': best.requires,
                'notes': best.notes,
            }

    # Add gadget quality assessment
    if report.gadget_quality:
        gq = report.gadget_quality
        result['gadget_quality'] = {
            'can_ret2libc': gq.can_ret2libc,
            'can_execve_rop': gq.can_execve_rop,
            'can_mprotect_shellcode': gq.can_mprotect_shellcode,
            'has_stack_pivot': gq.has_stack_pivot,
            'capabilities': {
                'control_rdi': gq.can_control_rdi,
                'control_rsi': gq.can_control_rsi,
                'control_rdx': gq.can_control_rdx,
                'control_rax': gq.can_control_rax,
                'syscall_ret': gq.has_syscall_ret,
            },
            'total_gadgets': gq.total_gadgets,
            'usable_gadgets': gq.usable_after_bad_byte_filter,
        }

    # Cross-reference viable techniques against all blockers
    viability = assess_technique_viability_from_result(result)
    result['viability'] = viability

    # Add exploitation path analysis for common vulnerability types
    # This tells the LLM what chains are actually viable vs blocked
    result['exploitation_paths'] = {}

    # Get verified %n status from the analysis we just ran
    # This is critical: empirical test overrides glibc version-based assumptions
    glibc_n_verified_working = None
    if report.glibc_mitigations and report.glibc_mitigations.format_n_verified:
        # We ran the empirical test - trust its result
        glibc_n_verified_working = not report.glibc_mitigations.format_n_disabled
    elif hasattr(report, 'glibc_n_disabled'):
        # Fall back to the report's glibc_n_disabled flag
        glibc_n_verified_working = not report.glibc_n_disabled

    common_vulns = ['format_string_vuln', 'stack_overflow_vuln', 'heap_overflow_vuln']
    if vuln_type:
        # Normalize vuln_type to match primitive names
        normalized = vuln_type.lower().replace('-', '_').replace(' ', '_')
        if not normalized.endswith('_vuln'):
            normalized += '_vuln'
        if normalized not in common_vulns:
            common_vulns.insert(0, normalized)

    # Get input constraints (null byte issues, etc.)
    input_constraints = result.get('constraints', {})

    # Get gadget quality info
    gadget_quality = result.get('gadget_quality', {})

    # Cache binary protections and glibc version to avoid redundant analysis
    # These are constant across all vulnerability types for a given binary
    cached_protections = result.get('protections', {})
    cached_glibc_version = result.get('glibc_version')

    for vuln in common_vulns:
        try:
            path_result = find_exploit_paths(
                vuln,
                str(binary_path) if binary_path else None,
                glibc_n_verified_working=glibc_n_verified_working,
                input_constraints=input_constraints,
                gadget_quality=gadget_quality,
                binary_protections=cached_protections,
                glibc_version=cached_glibc_version
            )
            result['exploitation_paths'][vuln] = {
                'verdict': path_result.get('verdict', 'UNKNOWN'),
                'verdict_reason': path_result.get('verdict_reason', ''),
                'paths_found': len(path_result.get('paths', [])),
                'summary': path_result.get('summary', ''),
                'blocked_primitives': path_result.get('blocked_primitives', []),
            }
            # Add detailed analysis from path_result
            if 'analysis' in path_result:
                analysis = path_result['analysis']
                result['exploitation_paths'][vuln]['chain_breaks'] = analysis.get('chain_breaks', [])
                result['exploitation_paths'][vuln]['partial_exploitation'] = analysis.get('partial_exploitation', [])
                result['exploitation_paths'][vuln]['what_would_help'] = analysis.get('what_would_help', [])
                result['exploitation_paths'][vuln]['bottom_line'] = analysis.get('bottom_line', '')
                # Include alternative targets and honest assessment for DIFFICULT verdicts
                if analysis.get('alternative_targets'):
                    result['exploitation_paths'][vuln]['alternative_targets'] = analysis['alternative_targets']
                if analysis.get('honest_assessment'):
                    result['exploitation_paths'][vuln]['honest_assessment'] = analysis['honest_assessment']
                # Include input constraints (null byte issues)
                if analysis.get('input_constraint'):
                    result['exploitation_paths'][vuln]['input_constraint'] = analysis['input_constraint']
                # Include gadget quality issues
                if analysis.get('gadget_quality'):
                    result['exploitation_paths'][vuln]['gadget_quality'] = analysis['gadget_quality']
        except Exception as e:
            logger.debug(f"Could not analyze paths for {vuln}: {e}")

    # Save to output directory if specified (internal detail - don't expose filenames)
    if output_dir:
        out_path = Path(output_dir)
        out_path.mkdir(parents=True, exist_ok=True)

        # Save human-readable summary
        summary_file = out_path / "exploit_feasibility.txt"
        summary_file.write_text(report.summary())

        # Save machine-readable context (internal use)
        try:
            context = report.to_context(binary_path=str(binary_path) if binary_path else None)
            context.save(str(out_path / "exploit_context.json"))
        except Exception as e:
            logger.debug(f"Could not save exploit context: {e}")

    return result


def check_exploit_viability(binary_path: str = None,
                            vuln_type: str = None) -> Tuple[bool, str]:
    """
    Quick check if exploitation is viable given mitigations.

    Args:
        binary_path: Path to target binary (optional)
        vuln_type: Vulnerability type (e.g., "format_string", "buffer_overflow")

    Returns:
        Tuple of (is_viable, reason)
        - is_viable: True if exploitation may be feasible
        - reason: Human-readable explanation
    """
    from .analyzer import FeasibilityAnalyzer
    from .vuln_types import ExploitabilityVerdict

    # Auto-select profile based on vulnerability type (web vulns skip memory mitigations)
    profile = _get_profile_for_vuln_type(vuln_type, binary_path)
    analyzer = FeasibilityAnalyzer(binary_path=binary_path, profile=profile)
    report = analyzer.full_analysis(vuln_type=vuln_type, extended=False)

    if report.verdict == ExploitabilityVerdict.UNLIKELY:
        reasons = "; ".join(report.blockers) if report.blockers else "No viable exploitation path"
        return False, f"Unlikely: {reasons}"

    elif report.verdict == ExploitabilityVerdict.DIFFICULT:
        reasons = "; ".join(report.blockers) if report.blockers else "Exploitation is difficult"
        return True, f"Difficult but possible: {reasons}"

    elif report.verdict in (ExploitabilityVerdict.EXPLOITABLE, ExploitabilityVerdict.LIKELY_EXPLOITABLE):
        if report.warnings:
            return True, f"Likely exploitable (warnings: {len(report.warnings)})"
        return True, "Exploitation appears feasible"

    else:
        # UNKNOWN verdict
        return True, "Viability unknown - manual analysis recommended"


def assess_technique_viability_from_result(result: Dict[str, Any]) -> Dict[str, Any]:
    """
    Cross-reference viable techniques against all detected blockers.

    Takes the result from analyze_binary() and filters the viable techniques
    based on ALL mitigations (not just architectural constraints).

    Note: This operates on the dict output from analyze_binary(). For operating
    on BinarySpecificAnalysis dataclass, use targets.assess_technique_viability().

    Args:
        result: Result dict from analyze_binary()

    Returns:
        Dictionary with filtered assessment:
        {
            'blocked': [{'technique': str, 'reason': str}, ...],
            'viable': [str, ...],
            'verdict': 'blocked' | 'limited' | 'viable',
            'summary': str
        }
    """
    blocked = []
    viable = []

    # Get architectural constraints
    constraints = result.get('constraints', {})
    arch_blocked = constraints.get('blocked_techniques', [])
    arch_viable = constraints.get('viable_techniques', [])

    # Add architecturally blocked techniques
    null_pos = constraints.get('null_byte_position', 6)
    for t in arch_blocked:
        blocked.append({'technique': t, 'reason': f'null bytes at position {null_pos}'})

    # Get other blockers from warnings/protections
    warnings = result.get('warnings', [])
    protections = result.get('protections', {})
    glibc_mits = result.get('glibc_mitigations', {})
    kernel_mits = result.get('kernel_mitigations', {})

    # Get runtime-verified %n status
    glibc_n_disabled = result.get('glibc_n_disabled')
    glibc_version = glibc_mits.get('version', 0.0)

    # Track additional requirements for techniques that need primitives
    additional_requirements = []

    # Cross-reference each viable technique against other blockers
    for technique in arch_viable:
        dominated = False
        reason = None
        tech_lower = technique.lower()

        # ─────────────────────────────────────────────────────────────────────
        # Format string checks
        # ─────────────────────────────────────────────────────────────────────
        if 'format string' in tech_lower and '%n' in tech_lower:
            if glibc_n_disabled is True:
                dominated = True
                reason = 'VERIFIED: %n disabled at runtime'
            elif glibc_n_disabled is None and glibc_version >= 2.38:
                n_disabled = any('%n' in w.lower() and 'disabled' in w.lower() for w in warnings)
                if n_disabled:
                    dominated = True
                    reason = 'glibc 2.38+ disables %n by default'

        # ─────────────────────────────────────────────────────────────────────
        # Hook overwrite checks (glibc 2.34+)
        # ─────────────────────────────────────────────────────────────────────
        if any(x in tech_lower for x in ['hook', '__malloc_hook', '__free_hook']):
            if glibc_mits.get('hooks_removed'):
                dominated = True
                reason = 'glibc 2.34+: __malloc_hook/__free_hook removed'

        # ─────────────────────────────────────────────────────────────────────
        # GOT overwrite checks
        # ─────────────────────────────────────────────────────────────────────
        if 'got' in tech_lower and protections.get('full_relro'):
            dominated = True
            reason = 'Full RELRO makes GOT read-only'

        # ─────────────────────────────────────────────────────────────────────
        # Heap exploitation checks
        # ─────────────────────────────────────────────────────────────────────
        if any(x in tech_lower for x in ['tcache', 'fastbin', 'heap']):
            # Safe-linking (2.31+) - doesn't block but requires heap leak
            if glibc_mits.get('safe_linking'):
                if 'poison' in tech_lower or 'tcache' in tech_lower:
                    additional_requirements.append(
                        f"{technique}: Requires heap address leak (safe-linking XORs fd pointers)"
                    )

        if 'double' in tech_lower and 'free' in tech_lower:
            # tcache key (2.29+) - doesn't block but requires key bypass
            if glibc_mits.get('tcache_key'):
                additional_requirements.append(
                    f"{technique}: Requires heap leak to forge tcache key, or UAF to clear it"
                )

        # ─────────────────────────────────────────────────────────────────────
        # FILE/FSOP checks
        # ─────────────────────────────────────────────────────────────────────
        if any(x in tech_lower for x in ['fsop', 'file', '_io_', 'vtable']):
            if glibc_mits.get('vtable_verification'):
                additional_requirements.append(
                    f"{technique}: vtable must be within __libc_IO_vtables (use _IO_str_overflow chain)"
                )

        # ─────────────────────────────────────────────────────────────────────
        # Exit handler checks
        # ─────────────────────────────────────────────────────────────────────
        if any(x in tech_lower for x in ['exit', 'atexit', '__exit_funcs']):
            if glibc_mits.get('pointer_mangling'):
                additional_requirements.append(
                    f"{technique}: Pointers are mangled (need TLS leak for pointer guard)"
                )

        # ─────────────────────────────────────────────────────────────────────
        # Kernel mitigation checks
        # ─────────────────────────────────────────────────────────────────────

        # NULL pointer dereference blocked by mmap_min_addr
        # Be specific to avoid false positives with "null byte" in technique names
        if any(x in tech_lower for x in ['null deref', 'null pointer', 'mmap(0)', 'page zero', 'map null']):
            mmap_min = kernel_mits.get('mmap_min_addr', 0)
            if mmap_min >= 65536:
                dominated = True
                reason = f'mmap_min_addr={mmap_min} prevents mapping NULL page'

        # BPF-based techniques blocked
        if any(x in tech_lower for x in ['bpf', 'ebpf']):
            if kernel_mits.get('unprivileged_bpf_disabled', 0) >= 1:
                dominated = True
                reason = 'unprivileged_bpf_disabled=1 blocks BPF exploits'

        # User namespace escape techniques
        if any(x in tech_lower for x in ['userns', 'user namespace', 'container escape']):
            if kernel_mits.get('unprivileged_userns_clone', 1) == 0:
                dominated = True
                reason = 'unprivileged_userns_clone=0 blocks user namespace exploits'

        # Kernel module loading
        if any(x in tech_lower for x in ['module', 'insmod', 'rootkit']):
            if kernel_mits.get('modules_disabled', 0) >= 1:
                dominated = True
                reason = 'modules_disabled=1 prevents kernel module loading'

        # ptrace-based techniques
        if any(x in tech_lower for x in ['ptrace', 'process injection', 'debugger']):
            ptrace_scope = kernel_mits.get('ptrace_scope', 0)
            if ptrace_scope >= 2:
                dominated = True
                reason = f'ptrace_scope={ptrace_scope} restricts ptrace'
            elif ptrace_scope == 1:
                additional_requirements.append(
                    f"{technique}: ptrace_scope=1 limits to descendants only"
                )

        # ASLR bypass requirements (affects all address-dependent techniques)
        if any(x in tech_lower for x in ['rop', 'ret2', 'jop', 'code reuse']):
            aslr = kernel_mits.get('aslr_level', 0)
            if aslr >= 2:
                additional_requirements.append(
                    f"{technique}: Full ASLR enabled - requires info leak first"
                )

        if dominated:
            blocked.append({'technique': technique, 'reason': reason})
        else:
            viable.append(technique)

    # Determine verdict
    if not viable:
        verdict = 'blocked'
        summary = 'BLOCKED - many exploitation techniques ruled out'
    elif len(viable) <= 2:
        verdict = 'limited'
        summary = f'LIMITED - only {len(viable)} technique(s) may work'
    else:
        verdict = 'viable'
        summary = f'VIABLE - {len(viable)} techniques available'

    return {
        'blocked': blocked,
        'viable': viable,
        'additional_requirements': additional_requirements,  # HARDER mitigations that need extra primitives
        'verdict': verdict,
        'summary': summary
    }


def get_exploit_constraints(arch: str = "x86_64",
                            input_handler: str = None) -> Dict[str, Any]:
    """
    Get architectural constraints for exploit development.

    Args:
        arch: Target architecture ("x86_64", "i386", etc.)
        input_handler: Input function used ("strcpy", "fgets", etc.)

    Returns:
        Dictionary with constraint information:
        {
            'strcpy_rop_viable': bool,
            'null_byte_position': int,
            'max_strcpy_bytes': int,
            'blocked_techniques': [str, ...],
            'viable_techniques': [str, ...],
            'reason': str,
        }
    """
    from .context import ExploitationConstraints

    ec = ExploitationConstraints(arch=arch, input_handler=input_handler or "")

    return {
        'arch': ec.arch,
        'strcpy_rop_viable': ec.strcpy_rop_viable,
        'null_byte_position': ec.null_byte_position,
        'max_strcpy_bytes': ec.max_strcpy_bytes,
        'blocked_techniques': list(ec.blocked_techniques),
        'viable_techniques': list(ec.viable_techniques),
        'reason': ec.strcpy_rop_reason,
    }


def get_vuln_type_for_rule(rule_id: str) -> Optional[str]:
    """
    Map a SARIF/scanner rule ID to a vulnerability type.

    Args:
        rule_id: Rule identifier from scanner (e.g., "cpp/format-string")

    Returns:
        Vulnerability type string or None if unknown
    """
    rule_lower = rule_id.lower()

    # Format string variants
    if any(x in rule_lower for x in ['sprintf', 'vsprintf']) and \
       any(x in rule_lower for x in ['overflow', 'buffer', 'oob']):
        return 'sprintf_overflow'

    if any(x in rule_lower for x in ['format-string-read', 'format_leak', 'printf-leak']):
        return 'format_string_read'

    if any(x in rule_lower for x in ['format-string', 'printf', 'format_string', 'formatstring']):
        return 'format_string_write'

    # Buffer overflows
    if any(x in rule_lower for x in ['stack-overflow', 'stack_buffer', 'stack-buffer']):
        return 'stack_buffer_overflow'

    if any(x in rule_lower for x in ['buffer-overflow', 'buffer_overflow', 'strcpy',
                                      'strcat', 'gets', 'memcpy']) and 'heap' not in rule_lower:
        return 'stack_buffer_overflow'

    if any(x in rule_lower for x in ['heap-overflow', 'heap_overflow', 'heap-buffer']):
        return 'heap_buffer_overflow'

    # Memory corruption
    if any(x in rule_lower for x in ['double-free', 'double_free']):
        return 'double_free'

    if any(x in rule_lower for x in ['use-after-free', 'uaf', 'use_after_free', 'dangling']):
        return 'use_after_free'

    if any(x in rule_lower for x in ['type-confusion', 'type_confusion']):
        return 'type_confusion'

    # Integer issues
    if any(x in rule_lower for x in ['integer-overflow', 'int-overflow', 'integer_overflow']):
        return 'integer_overflow'

    if any(x in rule_lower for x in ['signedness', 'sign-extension', 'sign_error']):
        return 'signedness_error'

    # Info leaks
    if any(x in rule_lower for x in ['info-leak', 'information-disclosure', 'oob-read']):
        return 'info_leak'

    if any(x in rule_lower for x in ['uninit', 'uninitialized']):
        return 'uninitialized_memory'

    # Other
    if any(x in rule_lower for x in ['null-deref', 'null-pointer', 'nullptr']):
        return 'null_dereference'

    if any(x in rule_lower for x in ['race', 'toctou', 'time-of-check']):
        return 'race_condition'

    # Web/remote (local mitigations don't apply)
    if any(x in rule_lower for x in ['sql-injection', 'sqli']):
        return 'sql_injection'

    if any(x in rule_lower for x in ['xss', 'cross-site-script']):
        return 'xss'

    if any(x in rule_lower for x in ['command-injection', 'shell-injection', 'os-command']):
        return 'command_injection'

    return None


def format_analysis_summary(result: Dict[str, Any], verbose: bool = False) -> str:
    """
    Format analysis result as user-friendly text.

    Args:
        result: Result dict from analyze_binary()
        verbose: Include detailed information

    Returns:
        Formatted string for display
    """
    lines = []
    lines.append("=" * 60)
    lines.append("MITIGATION ANALYSIS")
    lines.append("=" * 60)

    # Verdict
    verdict = result.get('verdict', 'unknown')
    verdict_icons = {
        'exploitable': '✓',
        'likely_exploitable': '~',
        'difficult': '⚠',
        'unlikely': '✗',
    }
    icon = verdict_icons.get(verdict, '?')
    lines.append(f"\nVerdict: {icon} {verdict.upper().replace('_', ' ')}")

    # Input handlers - show early as they drive constraint analysis
    input_handlers = result.get('input_handlers', [])
    constraints = result.get('constraints', {})
    if input_handlers or constraints.get('input_handler'):
        lines.append("\nInput Analysis:")
        if input_handlers:
            lines.append(f"  Detected handlers: {', '.join(input_handlers)}")
        if constraints.get('input_handler'):
            lines.append(f"  Primary constraint source: {constraints['input_handler']}")

    # Protections
    prots = result.get('protections', {})
    if prots:
        lines.append("\nProtections:")
        for name, enabled in sorted(prots.items()):
            status = "enabled" if enabled else "disabled"
            lines.append(f"  {name}: {status}")

    # Glibc mitigations - detailed view
    glibc_mits = result.get('glibc_mitigations', {})
    if glibc_mits:
        lines.append(f"\nGlibc Mitigations (v{glibc_mits.get('version', '?')}):")

        blockers = glibc_mits.get('blockers', [])
        if blockers:
            lines.append("  BLOCKED TECHNIQUES (use different approach):")
            for b in blockers:
                lines.append(f"    ✗ {b['name']}")
                if b.get('blocked_techniques'):
                    lines.append(f"      Blocks: {', '.join(b['blocked_techniques'][:2])}")
                if b.get('alternatives'):
                    lines.append(f"      Instead use: {', '.join(b['alternatives'][:2])}")

        primitive_reqs = glibc_mits.get('primitive_requirements', [])
        if primitive_reqs:
            lines.append("  REQUIRES PRIMITIVE FIRST (get this, then technique works):")
            for p in primitive_reqs:
                lines.append(f"    → {p['name']}")
                if p.get('required_primitive'):
                    lines.append(f"      Need: {p['required_primitive']}")
                elif p.get('bypass_requirements'):
                    lines.append(f"      Need: {p['bypass_requirements'][0]}")

        complications = glibc_mits.get('complications', [])
        if complications:
            lines.append("  COMPLICATES (be aware, may need adjustments):")
            for c in complications:
                lines.append(f"    ~ {c['name']}: {c['description']}")

    # Kernel mitigations - detailed view
    kernel_mits = result.get('kernel_mitigations', {})
    if kernel_mits and (kernel_mits.get('blockers') or kernel_mits.get('primitive_requirements') or kernel_mits.get('complications')):
        lines.append(f"\nKernel Mitigations ({kernel_mits.get('version', '?')}):")

        blockers = kernel_mits.get('blockers', [])
        if blockers:
            lines.append("  BLOCKED TECHNIQUES (use different approach):")
            for b in blockers:
                lines.append(f"    ✗ {b['name']} ({b.get('sysctl', '')}={b.get('value', '')})")
                if b.get('blocked_techniques'):
                    lines.append(f"      Blocks: {', '.join(b['blocked_techniques'][:2])}")
                if b.get('alternatives'):
                    lines.append(f"      Instead use: {', '.join(b['alternatives'][:2])}")

        primitive_reqs = kernel_mits.get('primitive_requirements', [])
        if primitive_reqs:
            lines.append("  REQUIRES PRIMITIVE FIRST (get this, then technique works):")
            for p in primitive_reqs[:5]:  # Limit to avoid spam
                lines.append(f"    → {p['name']} ({p.get('sysctl', '')}={p.get('value', '')})")
                if p.get('required_primitive'):
                    lines.append(f"      Need: {p['required_primitive']}")
                elif p.get('bypass_requirements'):
                    lines.append(f"      Need: {p['bypass_requirements'][0]}")
            if len(primitive_reqs) > 5:
                lines.append(f"    ... and {len(primitive_reqs) - 5} more")

        complications = kernel_mits.get('complications', [])
        if complications:
            lines.append("  COMPLICATES (be aware, may need adjustments):")
            for c in complications[:3]:  # Limit since these are less critical
                lines.append(f"    ~ {c['name']}: {c['description']}")
            if len(complications) > 3:
                lines.append(f"    ... and {len(complications) - 3} more")

    # Blockers
    blockers = result.get('blockers', [])
    if blockers:
        lines.append("\nBlockers:")
        for b in blockers:
            lines.append(f"  ✗ {b}")

    # Warnings
    warnings = result.get('warnings', [])
    if warnings and verbose:
        lines.append("\nWarnings:")
        for w in warnings:
            lines.append(f"  ⚠ {w}")

    # Technique viability - use cross-referenced assessment
    viability = result.get('viability', {})
    blocked = viability.get('blocked', [])
    viable = viability.get('viable', [])
    viability_verdict = viability.get('verdict', '')
    viability_summary = viability.get('summary', '')

    if blocked or viable or viability_verdict:
        lines.append("\n" + "-" * 40)
        lines.append("TECHNIQUE VIABILITY")
        lines.append("-" * 40)

        if viability_summary:
            verdict_icons = {'blocked': '✗', 'limited': '⚠', 'viable': '✓'}
            icon = verdict_icons.get(viability_verdict, '?')
            lines.append(f"\n  {icon} {viability_summary}")

        if not constraints.get('strcpy_rop_viable', True):
            lines.append(f"\n  Architecture: {constraints.get('arch', 'x86_64')}")
            lines.append(f"  Null byte position: {constraints.get('null_byte_position', 6)}")
            lines.append(f"  Max strcpy bytes: {constraints.get('max_strcpy_bytes', 6)}")

        if blocked:
            lines.append("\n  BLOCKED (don't waste time on these):")
            for item in blocked:
                if isinstance(item, dict):
                    lines.append(f"    ✗ {item['technique']} ({item['reason']})")
                else:
                    lines.append(f"    ✗ {item}")

        if viable:
            lines.append("\n  VIABLE (focus here):")
            for t in viable:
                lines.append(f"    ✓ {t}")
        elif viability_verdict == 'blocked':
            lines.append("\n  VIABLE: (none identified)")
            lines.append("\n  To proceed despite this:")
            lines.append("    → Modify environment (older glibc via Docker/chroot)")
            lines.append("    → Chain with other vulnerabilities in the target")
            lines.append("    → Analyze binary for unusual code paths or gadgets")

        # Show additional requirements for HARDER techniques
        additional_reqs = viability.get('additional_requirements', [])
        if additional_reqs:
            lines.append("\n  ADDITIONAL REQUIREMENTS (techniques need extra primitives):")
            for req in additional_reqs:
                lines.append(f"    ⚠ {req}")

    # Suggestions
    suggestions = result.get('suggestions', [])
    if suggestions:
        lines.append("\nSuggestions:")
        for s in suggestions[:5]:
            lines.append(f"  → {s}")

    # Exploitation Path Analysis - the key verdict for each vulnerability type
    exploit_paths = result.get('exploitation_paths', {})
    if exploit_paths:
        lines.append("\n" + "=" * 60)
        lines.append("EXPLOITATION PATH ANALYSIS")
        lines.append("=" * 60)

        for vuln, data in exploit_paths.items():
            verdict = data.get('verdict', 'UNKNOWN')
            # Map internal status to user-friendly descriptions
            verdict_display = {
                'NOT_EXPLOITABLE': ('✗', 'Unlikely'),
                'UNLIKELY': ('✗', 'Unlikely'),
                'DIFFICULT': ('⚠', 'Difficult'),
                'EXPLOITABLE': ('✓', 'Exploitable'),
                'LIKELY_EXPLOITABLE': ('✓', 'Likely exploitable'),
                'UNKNOWN': ('?', 'Unknown')
            }
            icon, description = verdict_display.get(verdict, ('?', verdict.replace('_', ' ').lower()))

            # Clean up vuln name for display (format_string_vuln -> Format string)
            vuln_display = vuln.replace('_vuln', '').replace('_', ' ').title()
            lines.append(f"\n{vuln_display}:")
            lines.append(f"  {icon} {description}")
            lines.append(f"    {data.get('verdict_reason', '')}")

            # Show alternative targets (with difficulty ratings)
            alt_targets = data.get('alternative_targets', [])
            if alt_targets:
                lines.append(f"\n  Alternative write targets (standard targets blocked):")
                for at in alt_targets:
                    difficulty = at.get('difficulty', '')
                    icon = '⚠' if 'HIGH' in difficulty else '~' if 'MEDIUM' in difficulty else '✓'
                    lines.append(f"    {icon} {at['target']}")
                    lines.append(f"      {at['description']}")
                    lines.append(f"      Technique: {at['technique']}")
                    lines.append(f"      Requirements: {at['requirements']}")
                    if difficulty:
                        lines.append(f"      Difficulty: {difficulty}")

            # Show honest assessment if available
            honest = data.get('honest_assessment', '')
            if honest:
                lines.append(f"\n  Reality check:")
                # Word wrap the assessment
                words = honest.split()
                line = "    "
                for word in words:
                    if len(line) + len(word) > 80:
                        lines.append(line)
                        line = "    " + word
                    else:
                        line += " " + word if line.strip() else word
                if line.strip():
                    lines.append(line)

            # Show input constraints (null byte issues) prominently
            input_constraint = data.get('input_constraint', {})
            if input_constraint:
                handler = input_constraint.get('handler', 'unknown')
                null_pos = input_constraint.get('null_byte_position', '?')
                blocked = input_constraint.get('blocked_techniques', [])
                viable = input_constraint.get('viable_alternatives', [])
                lines.append(f"\n  Input constraint ({handler}):")
                lines.append(f"    ✗ Null byte at position {null_pos} in 64-bit addresses")
                lines.append(f"    ✗ Cannot write full addresses via {handler}")
                if blocked:
                    lines.append(f"    Blocked: {', '.join(blocked[:3])}")
                if viable:
                    lines.append(f"    Still viable: {', '.join(viable[:3])}")

            # Show gadget quality issues
            gq = data.get('gadget_quality', {})
            if gq:
                usable = gq.get('usable_gadgets', 0)
                total = gq.get('total_gadgets', 0)
                missing = gq.get('missing_capabilities', [])
                lines.append(f"\n  ROP gadget quality:")
                lines.append(f"    ✗ Only {usable}/{total} gadgets usable in binary")
                lines.append(f"    ✗ Cannot build ROP chain from binary alone")
                if missing:
                    lines.append(f"    Missing: {', '.join(missing)}")
                lines.append(f"    → Must use libc gadgets (requires libc base leak)")

            # Show chain breaks for NOT_EXPLOITABLE
            chain_breaks = data.get('chain_breaks', [])
            if chain_breaks:
                lines.append(f"\n  Why exploitation is less likely:")
                for cb in chain_breaks:
                    lines.append(f"    ✗ {cb['break_point']}")
                    lines.append(f"      {cb['reason']}")

            # Show what's still possible
            partial = data.get('partial_exploitation', [])
            if partial:
                lines.append(f"\n  What you CAN still do:")
                for pe in partial:
                    lines.append(f"    ✓ {pe['primitive']}")
                    lines.append(f"      Technique: {pe['technique']}")
                    lines.append(f"      Reliability: {pe['reliability']}")

            # Show bottom line
            bottom_line = data.get('bottom_line', '')
            if bottom_line:
                lines.append(f"\n  Bottom line:")
                lines.append(f"    {bottom_line}")

            # Show what would help
            what_helps = data.get('what_would_help', [])
            if what_helps:
                lines.append(f"\n  To improve exploitability:")
                for wh in what_helps:
                    lines.append(f"    → {wh['change']}")
                    lines.append(f"      {wh['how']}")
                    # Show if this alone is sufficient or not
                    if 'enables' in wh:
                        lines.append(f"      ✓ Enables: {wh['enables']}")
                    elif 'removes_blocker' in wh:
                        lines.append(f"      ~ Removes: {wh['removes_blocker']}")
                        if 'still_blocked_by' in wh:
                            lines.append(f"      ✗ {wh['still_blocked_by']}")
                    if 'note' in wh:
                        lines.append(f"      Note: {wh['note']}")

    lines.append("\n" + "=" * 60)
    return "\n".join(lines)


# =============================================================================
# Primitive Dependency Graph API
# =============================================================================

def find_exploit_paths(
    vulnerability: str,
    binary_path: str = None,
    goal: str = "code_execution",
    glibc_n_verified_working: bool = None,
    input_constraints: Dict[str, Any] = None,
    gadget_quality: Dict[str, Any] = None,
    format_string_context: Dict[str, Any] = None,
    one_gadget_info: Dict[str, Any] = None,
    fini_array_context: Dict[str, Any] = None,
    binary_protections: Dict[str, Any] = None,
    glibc_version: str = None
) -> Dict[str, Any]:
    """
    Find exploitation paths from a vulnerability to a goal.

    This models how primitives chain together. The agent uses this to understand
    what sequence of steps leads from initial vulnerability to code execution.

    Args:
        vulnerability: Starting vulnerability type (e.g., "format_string_vuln",
                      "stack_overflow_vuln", "heap_overflow_vuln")
        binary_path: Path to binary (optional - used to infer mitigations)
        goal: Target goal (default: "code_execution")
        glibc_n_verified_working: Override for %n status from empirical test.
                                  If True, %n works despite glibc version.
                                  If False, %n is blocked.
                                  If None, infer from glibc version.
        input_constraints: Dict with input handler constraints (e.g., strcpy null byte issues).
                          Keys: input_handler, null_byte_position, blocked_techniques, etc.
        format_string_context: Dict with format string exploitation context.
                              Keys: call_count, single_shot, sinks
        one_gadget_info: Dict with one_gadget analysis.
                        Keys: has_partial_overwrite_gadget, best_partial_offset, gadgets
        fini_array_context: Dict with .fini_array partial overwrite feasibility.
                           Keys: partial_overwrite_viable, current_value_type, bytes_needed

    Returns:
        Dictionary with paths and analysis:
        {
            'paths': [
                {
                    'steps': ['format_string_vuln', 'format_string_read', 'libc_leak', 'ret2libc', 'code_execution'],
                    'reliability': 81.0,
                    'blocked_by': ['glibc_n_disabled'],
                    'complicated_by': ['aslr', 'stack_canary'],
                }
            ],
            'shortest_path': {...},
            'most_reliable_path': {...},
            'active_mitigations': ['full_relro', 'glibc_hooks_removed'],
            'blocked_primitives': ['format_string_write', 'hook_overwrite'],
            'summary': str
        }
    """
    from .analyzer import FeasibilityAnalyzer, create_dependency_graph

    # Get mitigations from binary analysis if not already provided
    # This avoids redundant analysis when called in a loop
    report = None  # May be set if we need to run analysis
    if binary_protections is None and binary_path:
        analyzer = FeasibilityAnalyzer(binary_path)
        report = analyzer.full_analysis(extended=False)
        binary_protections = dict(report.binary_protections)
        glibc_version = report.glibc_version

    # Create dependency graph with detected mitigations
    graph = create_dependency_graph(
        binary_protections=binary_protections,
        glibc_version=glibc_version
    )

    # Override %n status if empirically verified
    if glibc_n_verified_working is True:
        # Empirical test showed %n works - remove the blocker
        if 'glibc_n_disabled' in graph.active_mitigations:
            graph.active_mitigations.remove('glibc_n_disabled')
        # Re-enable format_string_write primitive
        if 'format_string_write' in graph.primitives:
            graph.primitives['format_string_write'].blocked_by = []
    elif glibc_n_verified_working is False:
        # Empirical test showed %n doesn't work - ensure blocker is set
        if 'glibc_n_disabled' not in graph.active_mitigations:
            graph.active_mitigations.add('glibc_n_disabled')
        if 'format_string_write' in graph.primitives:
            graph.primitives['format_string_write'].blocked_by = ['glibc_n_disabled']

    # Find all paths
    paths = graph.find_paths_to_goal(vulnerability, goal)

    # Build result
    result = {
        'paths': [],
        'shortest_path': None,
        'most_reliable_path': None,
        'active_mitigations': list(graph.active_mitigations),
        'blocked_primitives': [],
        'summary': ''
    }

    # Convert paths to dicts
    for p in paths:
        path_dict = {
            'steps': p.steps,
            'reliability': p.total_reliability,
            'blocked_by': p.blocked_mitigations,
            'complicated_by': p.complicating_mitigations,
            'notes': p.notes
        }
        # Add confidence data if available
        if p.confidence:
            path_dict['confidence'] = {
                'score': p.confidence.score,
                'level': p.confidence.level,
                'reasoning': p.confidence.reasoning,
                'adjustments': [
                    {'reason': r, 'delta': d}
                    for r, d in p.confidence.adjustments
                ]
            }
        result['paths'].append(path_dict)

    # Get shortest and most reliable
    if paths:
        shortest = min(paths, key=lambda x: len(x.steps))
        result['shortest_path'] = {
            'steps': shortest.steps,
            'reliability': shortest.total_reliability,
            'confidence': {
                'score': shortest.confidence.score,
                'level': shortest.confidence.level
            } if shortest.confidence else None
        }

        most_reliable = paths[0]  # Already sorted by reliability
        result['most_reliable_path'] = {
            'steps': most_reliable.steps,
            'reliability': most_reliable.total_reliability,
            'confidence': {
                'score': most_reliable.confidence.score,
                'level': most_reliable.confidence.level,
                'reasoning': most_reliable.confidence.reasoning
            } if most_reliable.confidence else None
        }

    # Find blocked primitives
    for name in graph.primitives:
        blocked, reason = graph.is_blocked(name)
        if blocked:
            result['blocked_primitives'].append({
                'name': name,
                'blocked_by': reason
            })

    # Generate verdict based on paths found
    # But also consider if key primitives are available even without modeled paths
    if not paths:
        # Check if format string write is actually available (verified %n working)
        # Even without GOT/hooks, there are alternative targets (.fini_array, __exit_funcs)
        fmt_write_available = (
            'format_string' in vulnerability and
            'glibc_n_disabled' not in graph.active_mitigations and
            'format_string_write' in graph.primitives and
            not graph.primitives['format_string_write'].blocked_by
        )

        if fmt_write_available:
            # %n works but standard targets (GOT, hooks) are blocked
            # Check what alternative targets are actually available
            has_full_relro = 'full_relro' in graph.active_mitigations

            alternative_targets = []

            # .fini_array is typically in the RELRO segment with standard linker scripts
            # This means Full RELRO makes it read-only (same as GOT)
            # Edge case: custom linker scripts could place it elsewhere, but this is rare
            if not has_full_relro:
                alternative_targets.append({
                    'target': '.fini_array',
                    'description': 'Array of function pointers called at program exit',
                    'technique': 'Overwrite .fini_array[0] with address of one_gadget or controlled function',
                    'requirements': 'PIE base leak + suitable gadget at predictable offset',
                    'difficulty': 'MEDIUM - need to find gadget that works with partial overwrite',
                })

            # __exit_funcs is in libc heap, not protected by RELRO but needs PTR_DEMANGLE bypass
            alternative_targets.append({
                'target': '__exit_funcs (libc)',
                'description': 'Linked list of exit handlers in libc heap',
                'technique': 'Overwrite function pointer in exit handler chain',
                'requirements': 'Libc base leak + PTR_DEMANGLE bypass (leak pointer guard from TLS)',
                'difficulty': 'HIGH - pointer mangling makes this very difficult',
            })

            # Stack return address (if we can write there)
            alternative_targets.append({
                'target': 'Stack return address',
                'description': 'Overwrite saved return address on stack',
                'technique': 'Use format string to write to stack location',
                'requirements': 'Stack address leak + exact offset calculation',
                'difficulty': 'MEDIUM - need stack leak and careful offset math',
            })

            if alternative_targets:
                result['verdict'] = 'DIFFICULT'
                if has_full_relro:
                    result['verdict_reason'] = '%n writes work, but Full RELRO blocks GOT AND .fini_array - very limited targets'
                else:
                    result['verdict_reason'] = '%n writes work, but GOT/hooks blocked - requires alternative targets'
                result['summary'] = (
                    f"Format string write primitive available, but standard targets blocked. "
                    f"{len(alternative_targets)} alternative target(s) with significant requirements."
                )
                result['analysis'] = {
                    'conclusion': 'Exploitation difficult - standard targets blocked, alternatives require extra work',
                    'critical_blockers': [],
                    'chain_breaks': [],
                    'what_would_help': [],
                    'alternative_targets': alternative_targets,
                    'honest_assessment': (
                        "While %n writes work, the practical path to code execution is very limited. "
                        + ("Full RELRO blocks BOTH GOT and .fini_array. " if has_full_relro else "GOT is read-only (Full RELRO). ")
                        + "Hooks are removed (glibc 2.34+). "
                        + "Remaining targets (__exit_funcs, stack) require additional leaks and are difficult. "
                        + "Consider: running in older environment (no RELRO), or using info leaks to chain with other vulns."
                    )
                }
            else:
                # No alternative targets at all
                result['verdict'] = 'UNLIKELY'
                result['verdict_reason'] = '%n writes work but no viable write targets remain'
                result['summary'] = "Format string write available but all known targets are protected"
                result['analysis'] = {
                    'conclusion': 'Write primitive exists but no viable targets',
                    'critical_blockers': [],
                    'chain_breaks': [],
                    'what_would_help': [],
                    'honest_assessment': (
                        "%n writes work but there's nowhere useful to write. "
                        "Full RELRO protects GOT and .fini_array. Hooks removed in glibc 2.34+. "
                        "Only option is running in an older environment without these protections."
                    )
                }
        else:
            result['verdict'] = 'NOT_EXPLOITABLE'
            result['verdict_reason'] = 'No viable exploitation paths found'
            result['summary'] = (
                f"No viable paths from {vulnerability} to {goal}. "
                f"Blocked by: {', '.join(graph.active_mitigations)}"
            )

            # Add detailed breakdown for disappointed users
            result['analysis'] = {
                'conclusion': 'Exploitation is not viable with current mitigations',
                'critical_blockers': [],
                'chain_breaks': [],
                'what_would_help': []
            }

        # Identify critical blockers - mitigations that block key primitives
        critical_mitigations = {}
        for bp in result['blocked_primitives']:
            blocker = bp['blocked_by']
            if blocker not in critical_mitigations:
                critical_mitigations[blocker] = []
            critical_mitigations[blocker].append(bp['name'])

        for mit, prims in critical_mitigations.items():
            result['analysis']['critical_blockers'].append({
                'mitigation': mit,
                'blocks': prims,
                'impact': f"Prevents {', '.join(prims)}"
            })

        # Explain chain breaks based on vulnerability type
        if 'format_string' in vulnerability:
            if 'glibc_n_disabled' in graph.active_mitigations:
                result['analysis']['chain_breaks'].append({
                    'break_point': 'format_string_vuln → format_string_write',
                    'reason': 'glibc 2.38+ disables %n format specifier at runtime',
                    'detail': 'Cannot write to arbitrary memory via format string - the core exploitation primitive is blocked'
                })
            if 'full_relro' in graph.active_mitigations:
                result['analysis']['chain_breaks'].append({
                    'break_point': 'arbitrary_write → got_overwrite',
                    'reason': 'Full RELRO makes GOT read-only after startup',
                    'detail': 'Even with a write primitive, GOT entries cannot be modified'
                })
                # CRITICAL: Full RELRO typically blocks .fini_array too (standard linker scripts
                # place it in the RELRO segment alongside GOT)
                result['analysis']['chain_breaks'].append({
                    'break_point': 'arbitrary_write → fini_array_overwrite',
                    'reason': 'Full RELRO typically makes .fini_array read-only (standard linker places it in RELRO segment)',
                    'detail': '.fini_array is NOT a viable alternative when Full RELRO is enabled - writes will crash'
                })
            if 'glibc_hooks_removed' in graph.active_mitigations:
                result['analysis']['chain_breaks'].append({
                    'break_point': 'arbitrary_write → hook_overwrite',
                    'reason': 'glibc 2.34+ removed __malloc_hook/__free_hook',
                    'detail': 'Classic hook overwrite technique no longer exists'
                })

            # NEW: Add format string call count constraint
            if format_string_context:
                call_count = format_string_context.get('call_count', 0)
                single_shot = format_string_context.get('single_shot', False)
                if single_shot or call_count == 1:
                    result['analysis']['chain_breaks'].append({
                        'break_point': 'format_string_write → chained_writes',
                        'reason': f'Only {call_count} printf call(s) in binary',
                        'detail': 'Single printf = single write opportunity. Cannot chain writes across '
                                  'multiple calls. Need to write complete target value in one shot, or '
                                  'use multiple %n in single format string (limited by format string length).'
                    })
                    result['analysis']['format_string_context'] = {
                        'call_count': call_count,
                        'single_shot': True,
                        'implication': 'Must use one-shot gadget or construct full address in single call'
                    }

            # NEW: Add write primitive constraint analysis
            if report and report.exploit_primitives:
                ep = report.exploit_primitives
                write_size = ep.write_size
                write_count = ep.write_count

                # Analyze write constraints impact
                if write_size and write_count:
                    result['analysis']['write_constraints'] = {
                        'write_size': write_size,
                        'write_count': write_count,
                    }

                    # Single-shot sequential overflow (strcpy/sprintf)
                    if write_count == 'once' and 'sequential' in write_size:
                        result['analysis']['chain_breaks'].append({
                            'break_point': 'buffer_overflow → arbitrary_address_overwrite',
                            'reason': 'Sequential overflow can only write contiguous bytes once',
                            'detail': 'Cannot skip bytes or write to multiple locations. '
                                      'Return address overwrite is possible, but multi-location writes (GOT entries) are not.'
                        })
                        result['analysis']['write_constraints']['limitation'] = 'contiguous_only'

                    # Format string single-byte writes need chaining for 8-byte targets
                    if '1' in write_size and 'byte' in write_size.lower():
                        result['analysis']['write_constraints']['note'] = (
                            'Single-byte writes (%hhn): Need 8 writes to construct 8-byte address. '
                            'Each %hhn writes value mod 256. Plan byte-by-byte construction carefully.'
                        )
                        # Calculate writes needed for 8-byte target
                        result['analysis']['write_constraints']['writes_for_pointer'] = 8

                    # Check if write_count supports necessary multi-writes
                    if write_count == 'single' and '1' in str(write_size):
                        result['analysis']['chain_breaks'].append({
                            'break_point': 'single_write → pointer_overwrite',
                            'reason': 'Single-byte write with only one write opportunity',
                            'detail': 'Cannot construct 8-byte pointer with single 1-byte write. '
                                      'Target partial overwrites (2 bytes) or find alternative technique.'
                        })

            # NEW: Add .fini_array partial overwrite constraint
            if fini_array_context:
                partial_viable = fini_array_context.get('partial_overwrite_viable', True)
                current_type = fini_array_context.get('current_value_type', '')
                bytes_needed = fini_array_context.get('bytes_needed', 0)
                if not partial_viable and current_type == 'pie_addr':
                    result['analysis']['chain_breaks'].append({
                        'break_point': 'fini_array_overwrite → one_gadget',
                        'reason': f'.fini_array contains PIE addr (0x55...), need libc addr (0x7f...) - {bytes_needed} bytes differ',
                        'detail': 'Cannot partial-overwrite a PIE address to become a libc address. '
                                  'The high bytes (0x55 vs 0x7f) are completely different. '
                                  'strcpy null-terminates at byte 6, so full 8-byte overwrite is blocked. '
                                  'Need format string multi-byte %n writes, or find binary gadget at PIE offset.'
                    })
                    result['analysis']['fini_array_constraint'] = {
                        'current_value_type': current_type,
                        'target_value_type': 'libc_addr',
                        'bytes_needed': bytes_needed,
                        'partial_overwrite_viable': False,
                        'alternative': 'Binary gadget at PIE-relative offset, or multi-byte %n writes'
                    }

            # NEW: Add one_gadget constraint analysis
            if one_gadget_info:
                best_offset = one_gadget_info.get('best_partial_offset', 0)
                bytes_needed = one_gadget_info.get('bytes_needed', 3)
                has_2byte = one_gadget_info.get('has_partial_overwrite_gadget', False)

                # Report the best one_gadget and bytes needed
                result['analysis']['one_gadget_info'] = {
                    'best_offset': best_offset,
                    'bytes_needed': bytes_needed,
                    'has_2byte_gadget': has_2byte,
                    'gadgets': one_gadget_info.get('gadgets', [])[:3]  # First 3 for reference
                }

                # Only add as chain break if no 2-byte partial overwrite option
                # (3+ bytes is usually achievable via format string %hn/%hhn)
                if not has_2byte and bytes_needed >= 3:
                    result['analysis']['one_gadget_info']['note'] = (
                        f'Best one_gadget at 0x{best_offset:x} needs {bytes_needed} bytes. '
                        f'2-byte partial overwrite not viable. Use %hn/%hhn for multi-byte writes.'
                    )

        elif 'stack_overflow' in vulnerability or 'buffer_overflow' in vulnerability:
            # Stack overflow specific chain breaks
            if 'stack_canary' in graph.active_mitigations:
                result['analysis']['chain_breaks'].append({
                    'break_point': 'stack_overflow_vuln → return_address_overwrite',
                    'reason': 'Stack canary detects buffer overflow before return',
                    'detail': 'Canary value is checked before function returns - overflow will crash, not redirect execution'
                })
            else:
                # No canary - but check other blockers
                if 'nx' in graph.active_mitigations and 'pie' in graph.active_mitigations:
                    if 'aslr' in graph.active_mitigations:
                        result['analysis']['chain_breaks'].append({
                            'break_point': 'return_address_overwrite → code_execution',
                            'reason': 'PIE + ASLR randomizes addresses, NX blocks shellcode',
                            'detail': 'Need info leak to defeat ASLR, then ROP chain (no direct shellcode)'
                        })
                # Check if we can actually build ROP
                if 'full_relro' in graph.active_mitigations:
                    result['analysis']['chain_breaks'].append({
                        'break_point': 'rop_chain → got_overwrite',
                        'reason': 'Full RELRO makes GOT read-only',
                        'detail': 'Cannot redirect execution via GOT - need direct ROP to libc or one_gadget'
                    })

            # Check for null byte constraints from input handler (e.g., strcpy)
            if input_constraints:
                input_handler = input_constraints.get('input_handler', '')
                null_pos = input_constraints.get('null_byte_position')
                blocked_techniques = input_constraints.get('blocked_techniques', [])

                if input_handler == 'strcpy' and null_pos:
                    result['analysis']['chain_breaks'].append({
                        'break_point': 'return_address_overwrite → ROP_chain',
                        'reason': f'strcpy stops at null byte (position {null_pos} in 64-bit addresses)',
                        'detail': f'On x86_64, addresses contain null bytes at position {null_pos}. '
                                  f'strcpy cannot write full 8-byte addresses. '
                                  f'Blocked: {", ".join(blocked_techniques[:3])}'
                    })
                    # Add to analysis for visibility
                    result['analysis']['input_constraint'] = {
                        'handler': input_handler,
                        'null_byte_position': null_pos,
                        'max_useful_bytes': null_pos,
                        'blocked_techniques': blocked_techniques,
                        'viable_alternatives': input_constraints.get('viable_techniques', [])
                    }

            # Check for gadget quality issues
            if gadget_quality:
                usable = gadget_quality.get('usable_gadgets', 0)
                can_ret2libc = gadget_quality.get('can_ret2libc', False)
                can_execve = gadget_quality.get('can_execve_rop', False)

                if usable == 0 or (not can_ret2libc and not can_execve):
                    missing = []
                    caps = gadget_quality.get('capabilities', {})
                    if not caps.get('control_rdi'):
                        missing.append('pop rdi')
                    if not caps.get('control_rsi'):
                        missing.append('pop rsi')
                    if not caps.get('control_rdx'):
                        missing.append('pop rdx')

                    result['analysis']['chain_breaks'].append({
                        'break_point': 'ROP_chain → code_execution',
                        'reason': f'Insufficient ROP gadgets in binary ({usable} usable)',
                        'detail': f'Cannot build ret2libc or execve ROP chain. '
                                  f'Missing: {", ".join(missing) if missing else "key gadgets"}. '
                                  f'Need to use libc gadgets instead (requires libc leak).'
                    })
                    result['analysis']['gadget_quality'] = {
                        'usable_gadgets': usable,
                        'total_gadgets': gadget_quality.get('total_gadgets', 0),
                        'can_ret2libc': can_ret2libc,
                        'can_execve_rop': can_execve,
                        'missing_capabilities': missing
                    }

        # Suggest what would help - vulnerability-specific
        # Count blockers to give accurate "will this work?" assessment
        format_string_blockers = []
        if 'format_string' in vulnerability:
            if 'glibc_n_disabled' in graph.active_mitigations:
                format_string_blockers.append('glibc_%n')
            if 'full_relro' in graph.active_mitigations:
                format_string_blockers.append('full_relro')
            if 'pie' in graph.active_mitigations and 'aslr' in graph.active_mitigations:
                format_string_blockers.append('aslr_pie')

            remaining_after_glibc = len(format_string_blockers) - (1 if 'glibc_%n' in format_string_blockers else 0)

            if 'glibc_n_disabled' in graph.active_mitigations:
                if remaining_after_glibc > 0:
                    result['analysis']['what_would_help'].append({
                        'change': 'Use older glibc (< 2.38)',
                        'how': 'Run in container with Ubuntu 20.04/22.04, or use LD_PRELOAD',
                        'removes_blocker': '%n write primitive',
                        'still_blocked_by': f'{remaining_after_glibc} other mitigation(s) - not sufficient alone'
                    })
                else:
                    result['analysis']['what_would_help'].append({
                        'change': 'Use older glibc (< 2.38)',
                        'how': 'Run in container with Ubuntu 20.04/22.04, or use LD_PRELOAD',
                        'enables': 'format_string_write → code execution'
                    })
            if 'full_relro' in graph.active_mitigations:
                result['analysis']['what_would_help'].append({
                    'change': 'Binary compiled without Full RELRO',
                    'how': 'Compile with -Wl,-z,norelro or -Wl,-z,relro (partial)',
                    'removes_blocker': 'GOT write protection',
                    'note': 'Requires recompilation - not always possible'
                })

        elif 'stack_overflow' in vulnerability or 'buffer_overflow' in vulnerability:
            if 'stack_canary' in graph.active_mitigations:
                result['analysis']['what_would_help'].append({
                    'change': 'Leak or brute-force the canary',
                    'how': 'Find format string bug to leak stack, or byte-by-byte brute force (forking servers)',
                    'enables': 'bypass canary check, enable return address overwrite'
                })
                result['analysis']['what_would_help'].append({
                    'change': 'Binary compiled without stack canary',
                    'how': 'Compile with -fno-stack-protector',
                    'enables': 'direct return address overwrite'
                })
            if 'pie' in graph.active_mitigations or 'aslr' in graph.active_mitigations:
                result['analysis']['what_would_help'].append({
                    'change': 'Leak binary/libc addresses',
                    'how': 'Find format string bug or other info leak',
                    'enables': 'ROP chain with known gadget addresses'
                })
            if 'nx' in graph.active_mitigations:
                result['analysis']['what_would_help'].append({
                    'change': 'Binary compiled without NX',
                    'how': 'Compile with -z execstack',
                    'enables': 'direct shellcode execution on stack'
                })

        if not result['analysis']['what_would_help']:
            result['analysis']['what_would_help'].append({
                'change': 'Find a different vulnerability class',
                'how': 'Look for heap corruption, use-after-free, or logic bugs',
                'enables': 'Alternative exploitation paths'
            })

        # Add "try anyway" section - what CAN still be done even without code execution
        result['analysis']['partial_exploitation'] = []

        if 'format_string' in vulnerability:
            # Format string read is almost always possible
            result['analysis']['partial_exploitation'].append({
                'primitive': 'Information Leak',
                'technique': 'Format string read with %p, %s, %x',
                'what_you_get': 'Stack addresses, heap addresses, libc addresses, canary values',
                'reliability': 'HIGH - %p/%x are not blocked by glibc',
                'useful_for': 'Bypassing ASLR for other vulnerabilities, canary leaks for stack overflows'
            })
            result['analysis']['partial_exploitation'].append({
                'primitive': 'Stack Disclosure',
                'technique': 'Read stack contents via %N$p positional parameters',
                'what_you_get': 'Return addresses, saved frame pointers, local variables',
                'reliability': 'HIGH',
                'useful_for': 'Understanding program state, finding other bugs'
            })
            if 'pie' in graph.active_mitigations:
                result['analysis']['partial_exploitation'].append({
                    'primitive': 'PIE Base Leak',
                    'technique': 'Leak return address and calculate PIE base',
                    'what_you_get': 'Binary base address, defeating PIE',
                    'reliability': 'HIGH',
                    'useful_for': 'If you find another write primitive later'
                })

        elif 'stack_overflow' in vulnerability or 'buffer_overflow' in vulnerability:
            # Stack overflow partial exploitation
            if 'stack_canary' in graph.active_mitigations:
                result['analysis']['partial_exploitation'].append({
                    'primitive': 'Denial of Service',
                    'technique': 'Overflow buffer to corrupt canary and crash',
                    'what_you_get': 'Reliable program crash / service disruption',
                    'reliability': 'HIGH - trivial to trigger',
                    'useful_for': 'DoS, triggering crash handlers, forcing restarts'
                })
                result['analysis']['partial_exploitation'].append({
                    'primitive': 'Local Variable Corruption',
                    'technique': 'Overflow into local variables before canary',
                    'what_you_get': 'Modify variables in same stack frame',
                    'reliability': 'MEDIUM - depends on stack layout',
                    'useful_for': 'Logic bugs, authentication bypass, path manipulation'
                })
            else:
                # No canary - more options
                result['analysis']['partial_exploitation'].append({
                    'primitive': 'Return Address Overwrite',
                    'technique': 'Overflow directly to saved RIP',
                    'what_you_get': 'Control over program counter',
                    'reliability': 'HIGH - no canary to bypass',
                    'useful_for': 'Need info leak for ASLR, then ROP'
                })
                if 'pie' in graph.active_mitigations or 'aslr' in graph.active_mitigations:
                    result['analysis']['partial_exploitation'].append({
                        'primitive': 'Partial Overwrite',
                        'technique': 'Overwrite only low bytes of return address',
                        'what_you_get': 'Jump within same page (256 byte range)',
                        'reliability': 'LOW - limited targets',
                        'useful_for': 'If useful gadget exists at similar offset'
                    })

        # Add bottom line based on vulnerability type
        if 'format_string' in vulnerability:
            result['analysis']['bottom_line'] = (
                "Modern glibc mitigations hinder code execution, but information disclosure is still possible. "
                "Leaked addresses could be valuable if you find another vulnerability "
                "that provides a write primitive (buffer overflow, heap corruption, etc.)."
            )
        elif 'stack_overflow' in vulnerability or 'buffer_overflow' in vulnerability:
            if 'stack_canary' in graph.active_mitigations:
                result['analysis']['bottom_line'] = (
                    "Stack canary hinders code execution. You can still crash the program "
                    "or corrupt local variables. Look for a format string bug to leak the canary, "
                    "or target variables before the canary in the stack frame."
                )
            else:
                result['analysis']['bottom_line'] = (
                    "Return address overwrite is possible but ASLR/PIE hinder reliable exploitation. "
                    "Need an info leak (format string, etc.) to defeat ASLR before building ROP chain."
                )
        else:
            result['analysis']['bottom_line'] = (
                "Active mitigations hinder exploitation. "
                "Look for additional vulnerabilities that could provide missing primitives."
            )
    elif len(paths) == 1:
        reliability = paths[0].total_reliability
        if reliability < 30:
            result['verdict'] = 'DIFFICULT'
            result['verdict_reason'] = f'Only 1 low-reliability path ({reliability:.0f}%)'
        else:
            result['verdict'] = 'EXPLOITABLE'
            result['verdict_reason'] = f'1 viable path at {reliability:.0f}% reliability'
        result['summary'] = (
            f"Found 1 path from {vulnerability} to {goal}. "
            f"Reliability: {reliability:.0f}% "
            f"({' → '.join(paths[0].steps)})"
        )
    elif len(paths) <= 3:
        best_reliability = result['most_reliable_path']['reliability']
        result['verdict'] = 'EXPLOITABLE'
        result['verdict_reason'] = f'{len(paths)} paths, best at {best_reliability:.0f}%'
        result['summary'] = (
            f"Found {len(paths)} path(s) from {vulnerability} to {goal}. "
            f"Most reliable: {best_reliability:.0f}% "
            f"({' → '.join(result['most_reliable_path']['steps'])})"
        )
    else:
        best_reliability = result['most_reliable_path']['reliability']
        result['verdict'] = 'LIKELY_EXPLOITABLE'
        result['verdict_reason'] = f'{len(paths)} paths available, best at {best_reliability:.0f}%'
        result['summary'] = (
            f"Found {len(paths)} path(s) from {vulnerability} to {goal}. "
            f"Most reliable: {best_reliability:.0f}% "
            f"({' → '.join(result['most_reliable_path']['steps'])})"
        )

    return result


def what_if_mitigation_blocked(
    mitigation: str,
    vulnerability: str = None
) -> Dict[str, Any]:
    """
    Analyze what happens if a mitigation blocks a specific primitive.

    Useful for understanding the impact of mitigations on exploit development.

    Args:
        mitigation: Mitigation to hypothetically add (e.g., "full_relro", "glibc_n_disabled")
        vulnerability: Optional starting vulnerability to focus analysis

    Returns:
        Dictionary with impact analysis:
        {
            'affected_vulnerabilities': {
                'format_string_vuln': [
                    {'steps': [...], 'reliability': 81.0, 'now_blocked': True}
                ]
            },
            'blocked_path_count': int,
            'summary': str
        }
    """
    from .analyzer import create_dependency_graph, PrimitiveDependencyGraph

    # Create graph without the mitigation
    graph = create_dependency_graph()

    # Run what-if analysis
    blocked_paths = graph.what_if_blocked(mitigation)

    result = {
        'affected_vulnerabilities': {},
        'blocked_path_count': 0,
        'summary': ''
    }

    # Filter to specific vulnerability if provided
    if vulnerability:
        if vulnerability in blocked_paths:
            affected_paths = blocked_paths[vulnerability]
            result['affected_vulnerabilities'][vulnerability] = [
                {
                    'steps': p.steps,
                    'reliability': p.total_reliability,
                    'now_blocked': True
                }
                for p in affected_paths
            ]
            result['blocked_path_count'] = len(affected_paths)
    else:
        # Show all affected vulnerabilities
        for vuln, paths in blocked_paths.items():
            result['affected_vulnerabilities'][vuln] = [
                {
                    'steps': p.steps,
                    'reliability': p.total_reliability,
                    'now_blocked': True
                }
                for p in paths
            ]
            result['blocked_path_count'] += len(paths)

    # Generate summary
    vuln_count = len(result['affected_vulnerabilities'])
    path_count = result['blocked_path_count']

    if path_count > 0:
        result['summary'] = (
            f"Adding '{mitigation}' would block {path_count} exploitation path(s) "
            f"from {vuln_count} vulnerability type(s)."
        )
    else:
        result['summary'] = f"Adding '{mitigation}' would not block any known paths."

    return result


def get_primitive_requirements(primitive: str) -> Dict[str, Any]:
    """
    Get detailed information about a primitive's requirements.

    Args:
        primitive: Primitive name (e.g., "ret2libc", "tcache_poison", "libc_leak")

    Returns:
        Dictionary with primitive details:
        {
            'name': str,
            'description': str,
            'type': 'vulnerability' | 'capability' | 'goal',
            'provides': [str, ...],
            'requires': [str, ...],
            'requires_any': [str, ...],
            'blocked_by': [str, ...],
            'complicated_by': [str, ...],
            'reliability': int,
            'notes': str
        }
    """
    from .primitives import get_primitive_definitions

    primitives = get_primitive_definitions()
    prim = primitives.get(primitive)
    if not prim:
        return {
            'error': f"Unknown primitive: {primitive}",
            'available': list(primitives.keys())
        }

    return {
        'name': prim.name,
        'description': prim.description,
        'type': prim.primitive_type,
        'provides': list(prim.provides),
        'requires': list(prim.requires),
        'requires_any': list(prim.requires_any),
        'blocked_by': list(prim.blocked_by),
        'complicated_by': list(prim.complicated_by),
        'reliability': prim.reliability,
        'notes': prim.notes
    }


# =============================================================================
# Context Persistence - Survives Context Compaction
# =============================================================================

def save_exploit_context(binary_path: str, output_dir: str = None) -> str:
    """
    Run feasibility analysis and save context to a file that survives compaction.

    This is the recommended way to start exploit development. It:
    1. Runs full feasibility analysis
    2. Saves a compact context file with all critical data
    3. Returns a short reference that should be kept in conversation

    Args:
        binary_path: Path to target binary
        output_dir: Where to save (default: same dir as binary)

    Returns:
        Path to saved context file

    Usage:
        >>> ctx_file = save_exploit_context('/path/to/vuln_binary')
        >>> print(f"Context saved to: {ctx_file}")
        >>> # After compaction, reload with:
        >>> ctx = load_exploit_context(ctx_file)
    """
    import json
    from pathlib import Path
    from datetime import datetime

    binary_path = Path(binary_path).resolve()
    if output_dir:
        out_dir = Path(output_dir)
    else:
        out_dir = binary_path.parent
    out_dir.mkdir(parents=True, exist_ok=True)

    # Run full analysis
    result = analyze_binary(str(binary_path), extended=True)

    # Create compact context with all essential data for exploitation
    context = {
        '_meta': {
            'binary': str(binary_path),
            'binary_name': binary_path.name,
            'created': datetime.now().isoformat(),
            'version': '1.1',  # Bumped for additional fields
        },
        'verdict': result.get('verdict'),
        'protections': result.get('protections', {}),
        'glibc_version': result.get('glibc_version'),
        'glibc_n_disabled': result.get('glibc_n_disabled'),
        'blockers': result.get('blockers', []),
        'warnings': result.get('warnings', []),
        'suggestions': result.get('suggestions', []),
        'constraints': result.get('constraints', {}),
    }

    # Add detected input handlers (critical for understanding constraints)
    if 'input_handlers' in result:
        context['input_handlers'] = result['input_handlers']

    # Add libc offsets (critical for exploitation)
    if 'libc' in result:
        context['libc'] = result['libc']

    # Add ROP gadgets (critical for exploitation)
    if 'rop_gadgets' in result:
        context['rop_gadgets'] = result['rop_gadgets']

    # Add gadget quality assessment (tells you what's achievable)
    if 'gadget_quality' in result:
        context['gadget_quality'] = result['gadget_quality']

    # Add format string context (critical for format string exploits)
    if 'format_string_context' in result:
        context['format_string_context'] = result['format_string_context']

    # Add viable targets from binary-specific analysis
    if 'binary_specific' in result:
        bs = result['binary_specific']
        context['viable_targets'] = bs.get('viable_targets', [])[:10]  # Top 10
        context['blocked_techniques'] = bs.get('blocked_techniques', {})
        context['viable_techniques'] = bs.get('viable_techniques', [])
        # Include best target recommendation
        if 'best_target' in bs:
            context['best_target'] = bs['best_target']

    # Add exploitation paths analysis (verdict per vuln type)
    if 'exploitation_paths' in result:
        context['exploitation_paths'] = result['exploitation_paths']

    # Add one-gadgets with constraints (already in libc, but highlight separately)
    if 'libc' in result and 'one_gadgets_detailed' in result['libc']:
        context['one_gadgets'] = result['libc']['one_gadgets_detailed']

    # Add 2-byte partial overwrite gadgets if available
    if 'libc_2byte_gadgets' in result:
        context['libc_2byte_gadgets'] = result['libc_2byte_gadgets']

    # Add glibc mitigations summary (tcache, safe_linking, etc.)
    if 'glibc_mitigations' in result:
        gm = result['glibc_mitigations']
        context['glibc_mitigations'] = {
            'tcache_key': gm.get('tcache_key'),
            'safe_linking': gm.get('safe_linking'),
            'hooks_removed': gm.get('hooks_removed'),
            'pointer_mangling': gm.get('pointer_mangling'),
        }

    # Add detailed input constraint analysis (blocked/viable/conditional)
    if 'input_constraint_analysis' in result:
        context['input_constraint_analysis'] = result['input_constraint_analysis']

    # Add libc fingerprinting potential
    if 'libc_fingerprinting' in result:
        context['libc_fingerprinting'] = result['libc_fingerprinting']

    # Add bad byte impact analysis for targets
    if 'bad_byte_analysis' in result:
        context['bad_byte_analysis'] = result['bad_byte_analysis']

    # Save to file
    ctx_file = out_dir / f"{binary_path.stem}_exploit_context.json"
    with open(ctx_file, 'w') as f:
        json.dump(context, f, indent=2, default=str)

    logger.info(f"Exploit context saved to: {ctx_file}")
    return str(ctx_file)


def load_exploit_context(context_file: str) -> Dict[str, Any]:
    """
    Load a previously saved exploit context.

    Use this after context compaction to recover critical data.

    Args:
        context_file: Path to saved context JSON file

    Returns:
        Dictionary with all saved context data

    Usage:
        >>> ctx = load_exploit_context('/path/to/vuln_binary_exploit_context.json')
        >>> print(f"Binary: {ctx['_meta']['binary_name']}")
        >>> print(f"Verdict: {ctx['verdict']}")
        >>> print(f"Libc system offset: {hex(ctx['libc']['system_offset'])}")
    """
    import json
    from pathlib import Path

    ctx_path = Path(context_file)
    if not ctx_path.exists():
        raise FileNotFoundError(f"Context file not found: {context_file}")

    with open(ctx_path) as f:
        context = json.load(f)

    logger.info(f"Loaded exploit context for: {context.get('_meta', {}).get('binary_name', 'unknown')}")
    return context


def print_exploit_context(context_file: str) -> str:
    """
    Load and print a human-readable summary of the exploit context.

    Use this after context compaction to quickly review critical data.

    Args:
        context_file: Path to saved context JSON file

    Returns:
        Formatted string with context summary
    """
    from .vuln_types import verdict_to_human

    ctx = load_exploit_context(context_file)

    lines = []
    meta = ctx.get('_meta', {})
    lines.append(f"=== EXPLOIT CONTEXT: {meta.get('binary_name', 'unknown')} ===")
    lines.append(f"Binary: {meta.get('binary', 'unknown')}")
    raw_verdict = ctx.get('verdict', 'unknown')
    lines.append(f"Verdict: {verdict_to_human(raw_verdict)}")
    lines.append("")

    # Protections
    prots = ctx.get('protections', {})
    prot_str = []
    if prots.get('full_relro'):
        prot_str.append('Full RELRO')
    elif prots.get('relro'):
        prot_str.append('Partial RELRO')
    if prots.get('pie'):
        prot_str.append('PIE')
    if prots.get('nx'):
        prot_str.append('NX')
    if prots.get('canary'):
        prot_str.append('Canary')
    lines.append(f"Protections: {', '.join(prot_str) if prot_str else 'None'}")

    # Glibc info
    glibc_ver = ctx.get('glibc_version')
    if glibc_ver:
        n_status = "DISABLED" if ctx.get('glibc_n_disabled') else "working"
        lines.append(f"Glibc: {glibc_ver} (%n {n_status})")

    # Glibc mitigations (heap exploitation relevant)
    gm = ctx.get('glibc_mitigations', {})
    if gm:
        mits = []
        if gm.get('safe_linking'):
            mits.append('safe_linking')
        if gm.get('tcache_key'):
            mits.append('tcache_key')
        if gm.get('hooks_removed'):
            mits.append('hooks_removed')
        if gm.get('pointer_mangling'):
            mits.append('ptr_mangle')
        if mits:
            lines.append(f"Heap mitigations: {', '.join(mits)}")

    # Input handlers detected
    handlers = ctx.get('input_handlers', [])
    if handlers:
        lines.append(f"Input handlers: {', '.join(handlers)}")

    # Constraints summary
    constraints = ctx.get('constraints', {})
    if constraints:
        arch = constraints.get('arch', 'unknown')
        null_pos = constraints.get('null_byte_position')
        strcpy_rop = constraints.get('strcpy_rop_viable')
        lines.append(f"Arch: {arch}" + (f", null@byte{null_pos}" if null_pos else ""))
        if strcpy_rop is False:
            lines.append("  [!] strcpy input - ROP NOT viable (null byte truncation)")

    # Blockers
    blockers = ctx.get('blockers', [])
    if blockers:
        lines.append(f"\nBlockers ({len(blockers)}):")
        for b in blockers[:5]:
            lines.append(f"  - {b}")

    # Suggestions (what might work)
    suggestions = ctx.get('suggestions', [])
    if suggestions:
        lines.append(f"\nSuggestions:")
        for s in suggestions[:3]:
            lines.append(f"  + {s}")

    # Format string context (critical for fmt exploits)
    fmt_ctx = ctx.get('format_string_context', {})
    if fmt_ctx:
        lines.append(f"\nFormat String Context:")
        lines.append(f"  printf calls: {fmt_ctx.get('call_count', 'unknown')}")
        if fmt_ctx.get('single_shot'):
            lines.append("  [!] SINGLE SHOT - must exploit in one printf call")
        if fmt_ctx.get('sinks'):
            lines.append(f"  sinks: {', '.join(fmt_ctx['sinks'].keys())}")

    # Libc offsets
    libc = ctx.get('libc', {})
    if libc:
        lines.append(f"\nLibc Offsets:")
        if libc.get('system_offset'):
            lines.append(f"  system: {hex(libc['system_offset'])}")
        if libc.get('bin_sh_offset'):
            lines.append(f"  /bin/sh: {hex(libc['bin_sh_offset'])}")

    # One-gadgets
    one_gadgets = ctx.get('one_gadgets', [])
    if one_gadgets:
        lines.append(f"\nOne-Gadgets ({len(one_gadgets)}):")
        for og in one_gadgets[:3]:
            lines.append(f"  {hex(og['offset'])}: {og.get('constraints', 'no constraints')}")

    # ROP gadgets
    rop = ctx.get('rop_gadgets', {})
    if rop:
        lines.append(f"\nROP Gadgets (total: {rop.get('total', 0)}, usable: {rop.get('usable', 0)}):")
        for name in ['pop_rdi', 'pop_rsi', 'pop_rdx', 'ret']:
            if name in rop:
                lines.append(f"  {name}: {hex(rop[name])}")

    # Gadget quality (what can we actually do?)
    gq = ctx.get('gadget_quality', {})
    if gq:
        caps = []
        if gq.get('can_ret2libc'):
            caps.append('ret2libc')
        if gq.get('can_execve_rop'):
            caps.append('execve_rop')
        if gq.get('can_mprotect_shellcode'):
            caps.append('mprotect+shellcode')
        if gq.get('has_stack_pivot'):
            caps.append('stack_pivot')
        if caps:
            lines.append(f"  Capabilities: {', '.join(caps)}")

    # Best target recommendation
    best = ctx.get('best_target')
    if best:
        lines.append(f"\nBest Target: {best.get('name')} @ {best.get('address')}")
        if best.get('notes'):
            lines.append(f"  {best['notes']}")

    # Viable targets
    targets = ctx.get('viable_targets', [])
    if targets:
        lines.append(f"\nViable Targets ({len(targets)}):")
        for t in targets[:5]:
            line = f"  {t['name']} @ {t['address']} ({t['technique']})"
            if t.get('partial_overwrite_viable'):
                line += " [partial OK]"
            lines.append(line)

    # Input constraint analysis (detailed blocked/viable/conditional)
    ica = ctx.get('input_constraint_analysis', {})
    if ica:
        lines.append(f"\nInput Constraints ({ica.get('handler', 'unknown')}):")
        if ica.get('blocked'):
            lines.append("  Blocked:")
            for b in ica['blocked'][:3]:
                lines.append(f"    - {b}")
        if ica.get('viable'):
            lines.append("  Still viable:")
            for v in ica['viable'][:3]:
                lines.append(f"    + {v}")
        if ica.get('conditional'):
            lines.append("  Conditional:")
            for c in ica['conditional'][:2]:
                lines.append(f"    ~ {c}")

    # Libc fingerprinting potential
    lfp = ctx.get('libc_fingerprinting', {})
    if lfp:
        lines.append(f"\nLibc Fingerprinting: {lfp.get('feasibility', 'unknown').title()}")
        lines.append(f"  Unique symbols: {lfp.get('symbol_count', 0)}")
        if lfp.get('notes'):
            for note in lfp['notes'][:2]:
                lines.append(f"  {note}")

    # Bad byte impact analysis
    bba = ctx.get('bad_byte_analysis', [])
    if bba:
        lines.append(f"\nBad Byte Impact ({len(bba)} targets affected):")
        for impact in bba[:3]:
            status = "FATAL" if impact.get('fatal') else "WORKABLE"
            lines.append(f"  {impact['target_name']} @ {impact['address_hex']}: {status}")
            if impact.get('workaround'):
                lines.append(f"    {impact['workaround']}")

    # Exploitation paths
    paths = ctx.get('exploitation_paths', {})
    if paths:
        lines.append(f"\nExploitation Paths:")
        for vuln, info in paths.items():
            raw_path_verdict = info.get('verdict', 'UNKNOWN')
            # Convert vuln name to readable (format_string_vuln -> format string)
            vuln_display = vuln.replace('_vuln', '').replace('_', ' ')
            lines.append(f"  {vuln_display}: {verdict_to_human(raw_path_verdict)}")
            if info.get('chain_breaks'):
                for cb in info['chain_breaks'][:2]:
                    lines.append(f"    - {cb}")
            if info.get('what_would_help'):
                for wh in info['what_would_help'][:2]:
                    lines.append(f"    + {wh}")

    return '\n'.join(lines)


def list_primitives(primitive_type: str = None) -> Dict[str, Any]:
    """
    List all available primitives.

    Args:
        primitive_type: Filter by type ('vulnerability', 'capability', 'goal')

    Returns:
        Dictionary with primitives grouped by type:
        {
            'vulnerabilities': [{'name': str, 'description': str}, ...],
            'capabilities': [...],
            'goals': [...]
        }
    """
    from .primitives import get_primitive_definitions

    primitives = get_primitive_definitions()

    result = {
        'vulnerabilities': [],
        'capabilities': [],
        'goals': []
    }

    for name, prim in primitives.items():
        entry = {
            'name': name,
            'description': prim.description,
            'provides': list(prim.provides),
            'reliability': prim.reliability
        }

        if prim.primitive_type == 'vulnerability':
            result['vulnerabilities'].append(entry)
        elif prim.primitive_type == 'capability':
            result['capabilities'].append(entry)
        elif prim.primitive_type == 'goal':
            result['goals'].append(entry)

    # Filter if requested
    if primitive_type:
        key = primitive_type + 's' if not primitive_type.endswith('s') else primitive_type
        if key in result:
            return {key: result[key]}

    return result
